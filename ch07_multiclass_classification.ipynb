{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNbwh8iMNj2HOxIfnFikGC7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/YasudaKaito/pytorch_book_akaishi/blob/main/ch07_multiclass_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7\n",
        "\n",
        "二値分類→多値分類で変わる点\n",
        "\n",
        "- 出力: 1次元 → 分類クラス数N次元\n",
        "    - 複数の分類器ができる\n",
        "- 活性化関数: sigmoid → softmax\n",
        "- 損失関数: 交差エントロピーの形式が変わる(下記)。\n",
        "    - 分類クラス数が全部でN個\n",
        "    - $yt_i$は正解のとき1,正解でないとき0\n",
        "    - $yp_i$はsoftmaxの出力ベクトルの要素\n",
        "\n",
        "$$\n",
        "\\sum_{i=0}^{N-1}(yt_i \\log yp_i)\n",
        "$$\n",
        "\n",
        "- PyTorchのポリシーとして「対数関数と指数関数をセットで使うべき」があり、`CrossEntroyLoss`には softmax -> 交差エントロピーが一体になっている"
      ],
      "metadata": {
        "id": "7s9cRKASR0B1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "iris = load_iris()"
      ],
      "metadata": {
        "id": "1jfMSnvyS8nC"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_org, y_org = iris.data, iris.target\n",
        "print(x_org.shape)\n",
        "print(y_org.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u7uEUBB_YQvX",
        "outputId": "a8756a26-beec-49bf-dfab-5fcfc3fead2a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(150, 4)\n",
            "(150,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# sepal がく片 length と petal 花弁 length のみ抽出\n",
        "x_select = x_org[:, [0, 2]]\n",
        "x_select.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MShLZOvqYSF8",
        "outputId": "f3fb055e-d464-40b3-d57c-5ca6bd127272"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(150, 2)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "lwFfyT8dZFRu"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train, x_test, y_train, y_test = train_test_split(x_select, y_org, train_size=75, test_size=75, random_state=123)\n",
        "print(x_train.shape, x_test.shape, y_train.shape, y_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "omfEWq9NZSBN",
        "outputId": "128e27fa-5b3b-46d5-f3b4-df5eb5d2a264"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(75, 2) (75, 2) (75,) (75,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "ff6y1WkUZwDO"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r6vOWpn7aF_e",
        "outputId": "d67cd988-15e5-40ff-90d3-8993d264ba2b"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 1, 0, 2, 2])"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 訓練データを正解ラベルごとにグループ分けして散布図\n",
        "x_t0 = x_train[y_train == 0]\n",
        "x_t1 = x_train[y_train == 1]\n",
        "x_t2 = x_train[y_train == 2]\n",
        "print(x_t0.shape, x_t1.shape, x_t2.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O6RNlC5nZ3Zy",
        "outputId": "94d91b5b-bfe9-4fee-d797-e96f329da98d"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(23, 2) (30, 2) (22, 2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.scatter(x_t0[:, 0], x_t0[:, 1], marker=\"x\", c=\"k\", s=50, label=\"0(setosa)\")\n",
        "plt.scatter(x_t1[:, 0], x_t1[:, 1], marker=\"o\", c=\"b\", s=50, label=\"1(versicolor)\")\n",
        "plt.scatter(x_t2[:, 0], x_t2[:, 1], marker=\"+\", c=\"k\", s=50, label=\"2(virginica)\")\n",
        "plt.xlabel(\"sepal_length\")\n",
        "plt.ylabel(\"petal_length\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 450
        },
        "id": "e6StfTdbaMI-",
        "outputId": "a09ae87d-5cf8-484a-b424-66db41bd72b4"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# モデル定義\n",
        "n_input = x_train.shape[1]\n",
        "n_output = len(set(y_train))\n",
        "print(n_input, n_output)"
      ],
      "metadata": {
        "id": "zlSZIlXGcnDb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "30fbc8b2-b6e5-4f71-9009-5e2dc72da644"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2 3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "# 入力次元2, 出力次元3のロジスティック回帰モデル\n",
        "class Net(nn.Module):\n",
        "    def __init__(self, n_input, n_output) -> None:\n",
        "        super().__init__()\n",
        "        self.l1 = nn.Linear(n_input, n_output)\n",
        "        # 初期値は1\n",
        "        self.l1.weight.data.fill_(1.0)\n",
        "        self.l1.bias.data.fill_(1.0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.l1(x)\n",
        "\n",
        "net = Net(n_input, n_output)"
      ],
      "metadata": {
        "id": "QAUl6Esn_0eI"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "lr = 0.01\n",
        "optimizer = optim.SGD(net.parameters(), lr=lr)"
      ],
      "metadata": {
        "id": "CKEIEy3EAn-f"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# shape: (75, 2)\n",
        "inputs = torch.tensor(x_train).float()\n",
        "# CrossEntropyLossの第二引数は整数型である必要がある\n",
        "labels = torch.tensor(y_train).long()\n",
        "inputs_test = torch.tensor(x_test).float()\n",
        "labels_test = torch.tensor(y_test).long()"
      ],
      "metadata": {
        "id": "v9WfOdxrBWbv"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "num_epochs = 10000\n",
        "history = np.zeros((0, 5))"
      ],
      "metadata": {
        "id": "TAGT0RUNB6IN"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(num_epochs):\n",
        "    # 訓練\n",
        "    optimizer.zero_grad()\n",
        "    # shape: (75, 3)\n",
        "    outputs = net(inputs)\n",
        "    loss = criterion(outputs, labels)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # 列方向にmaxを集計 出力: (75)\n",
        "    predicted = torch.max(outputs, 1)[1]\n",
        "    train_loss = loss.item()\n",
        "    train_acc = (predicted == labels).sum() / len(labels)\n",
        "\n",
        "    # 予測\n",
        "    outputs_test = net(inputs_test)\n",
        "    loss_test = criterion(outputs_test, labels_test)\n",
        "    predicted_test = torch.max(outputs_test, 1)[1]\n",
        "    val_loss = loss_test.item()\n",
        "    val_acc = (predicted_test == labels_test).sum() / len(labels_test)\n",
        "\n",
        "    if (epoch % 10 == 0):\n",
        "        print(f\"Epoch [{epoch}/{num_epochs}], loss: {train_loss:.5f} acc: {train_acc:.5f} val_loss: {val_loss:.5f} val_acc: {val_acc:.5f}\")\n",
        "        item = np.array([epoch, train_loss, train_acc, val_loss, val_acc])\n",
        "        history = np.vstack((history, item))"
      ],
      "metadata": {
        "id": "LfHIJwpuGebb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fe0bd0ef-29da-4859-f66d-843333928cde"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [0/10000], loss: 1.09861 acc: 0.30667 val_loss: 1.09263 val_acc: 0.26667\n",
            "Epoch [10/10000], loss: 1.03580 acc: 0.40000 val_loss: 1.06403 val_acc: 0.26667\n",
            "Epoch [20/10000], loss: 1.00477 acc: 0.40000 val_loss: 1.03347 val_acc: 0.26667\n",
            "Epoch [30/10000], loss: 0.97672 acc: 0.40000 val_loss: 1.00264 val_acc: 0.26667\n",
            "Epoch [40/10000], loss: 0.95057 acc: 0.41333 val_loss: 0.97351 val_acc: 0.26667\n",
            "Epoch [50/10000], loss: 0.92616 acc: 0.48000 val_loss: 0.94631 val_acc: 0.38667\n",
            "Epoch [60/10000], loss: 0.90338 acc: 0.69333 val_loss: 0.92098 val_acc: 0.56000\n",
            "Epoch [70/10000], loss: 0.88212 acc: 0.70667 val_loss: 0.89740 val_acc: 0.60000\n",
            "Epoch [80/10000], loss: 0.86227 acc: 0.70667 val_loss: 0.87545 val_acc: 0.61333\n",
            "Epoch [90/10000], loss: 0.84373 acc: 0.70667 val_loss: 0.85500 val_acc: 0.62667\n",
            "Epoch [100/10000], loss: 0.82640 acc: 0.70667 val_loss: 0.83594 val_acc: 0.62667\n",
            "Epoch [110/10000], loss: 0.81019 acc: 0.72000 val_loss: 0.81815 val_acc: 0.62667\n",
            "Epoch [120/10000], loss: 0.79500 acc: 0.72000 val_loss: 0.80153 val_acc: 0.62667\n",
            "Epoch [130/10000], loss: 0.78077 acc: 0.73333 val_loss: 0.78599 val_acc: 0.62667\n",
            "Epoch [140/10000], loss: 0.76741 acc: 0.74667 val_loss: 0.77142 val_acc: 0.64000\n",
            "Epoch [150/10000], loss: 0.75485 acc: 0.74667 val_loss: 0.75777 val_acc: 0.65333\n",
            "Epoch [160/10000], loss: 0.74303 acc: 0.74667 val_loss: 0.74494 val_acc: 0.68000\n",
            "Epoch [170/10000], loss: 0.73189 acc: 0.76000 val_loss: 0.73288 val_acc: 0.70667\n",
            "Epoch [180/10000], loss: 0.72138 acc: 0.77333 val_loss: 0.72151 val_acc: 0.76000\n",
            "Epoch [190/10000], loss: 0.71145 acc: 0.82667 val_loss: 0.71079 val_acc: 0.78667\n",
            "Epoch [200/10000], loss: 0.70205 acc: 0.82667 val_loss: 0.70067 val_acc: 0.78667\n",
            "Epoch [210/10000], loss: 0.69315 acc: 0.84000 val_loss: 0.69109 val_acc: 0.80000\n",
            "Epoch [220/10000], loss: 0.68470 acc: 0.84000 val_loss: 0.68202 val_acc: 0.80000\n",
            "Epoch [230/10000], loss: 0.67667 acc: 0.86667 val_loss: 0.67341 val_acc: 0.81333\n",
            "Epoch [240/10000], loss: 0.66904 acc: 0.86667 val_loss: 0.66524 val_acc: 0.81333\n",
            "Epoch [250/10000], loss: 0.66176 acc: 0.86667 val_loss: 0.65746 val_acc: 0.82667\n",
            "Epoch [260/10000], loss: 0.65483 acc: 0.85333 val_loss: 0.65005 val_acc: 0.82667\n",
            "Epoch [270/10000], loss: 0.64820 acc: 0.85333 val_loss: 0.64299 val_acc: 0.82667\n",
            "Epoch [280/10000], loss: 0.64187 acc: 0.85333 val_loss: 0.63625 val_acc: 0.82667\n",
            "Epoch [290/10000], loss: 0.63581 acc: 0.86667 val_loss: 0.62980 val_acc: 0.82667\n",
            "Epoch [300/10000], loss: 0.63000 acc: 0.88000 val_loss: 0.62363 val_acc: 0.82667\n",
            "Epoch [310/10000], loss: 0.62443 acc: 0.89333 val_loss: 0.61772 val_acc: 0.82667\n",
            "Epoch [320/10000], loss: 0.61909 acc: 0.89333 val_loss: 0.61205 val_acc: 0.82667\n",
            "Epoch [330/10000], loss: 0.61394 acc: 0.89333 val_loss: 0.60661 val_acc: 0.82667\n",
            "Epoch [340/10000], loss: 0.60900 acc: 0.89333 val_loss: 0.60138 val_acc: 0.84000\n",
            "Epoch [350/10000], loss: 0.60423 acc: 0.89333 val_loss: 0.59635 val_acc: 0.84000\n",
            "Epoch [360/10000], loss: 0.59964 acc: 0.90667 val_loss: 0.59150 val_acc: 0.85333\n",
            "Epoch [370/10000], loss: 0.59521 acc: 0.92000 val_loss: 0.58683 val_acc: 0.86667\n",
            "Epoch [380/10000], loss: 0.59093 acc: 0.92000 val_loss: 0.58232 val_acc: 0.86667\n",
            "Epoch [390/10000], loss: 0.58679 acc: 0.92000 val_loss: 0.57797 val_acc: 0.86667\n",
            "Epoch [400/10000], loss: 0.58279 acc: 0.92000 val_loss: 0.57377 val_acc: 0.86667\n",
            "Epoch [410/10000], loss: 0.57891 acc: 0.92000 val_loss: 0.56970 val_acc: 0.86667\n",
            "Epoch [420/10000], loss: 0.57516 acc: 0.92000 val_loss: 0.56576 val_acc: 0.86667\n",
            "Epoch [430/10000], loss: 0.57152 acc: 0.90667 val_loss: 0.56195 val_acc: 0.86667\n",
            "Epoch [440/10000], loss: 0.56799 acc: 0.90667 val_loss: 0.55825 val_acc: 0.86667\n",
            "Epoch [450/10000], loss: 0.56456 acc: 0.90667 val_loss: 0.55466 val_acc: 0.86667\n",
            "Epoch [460/10000], loss: 0.56123 acc: 0.90667 val_loss: 0.55118 val_acc: 0.86667\n",
            "Epoch [470/10000], loss: 0.55799 acc: 0.90667 val_loss: 0.54779 val_acc: 0.88000\n",
            "Epoch [480/10000], loss: 0.55484 acc: 0.90667 val_loss: 0.54451 val_acc: 0.88000\n",
            "Epoch [490/10000], loss: 0.55177 acc: 0.90667 val_loss: 0.54131 val_acc: 0.88000\n",
            "Epoch [500/10000], loss: 0.54878 acc: 0.90667 val_loss: 0.53819 val_acc: 0.88000\n",
            "Epoch [510/10000], loss: 0.54587 acc: 0.90667 val_loss: 0.53516 val_acc: 0.88000\n",
            "Epoch [520/10000], loss: 0.54303 acc: 0.90667 val_loss: 0.53221 val_acc: 0.88000\n",
            "Epoch [530/10000], loss: 0.54026 acc: 0.90667 val_loss: 0.52933 val_acc: 0.88000\n",
            "Epoch [540/10000], loss: 0.53755 acc: 0.90667 val_loss: 0.52652 val_acc: 0.88000\n",
            "Epoch [550/10000], loss: 0.53491 acc: 0.90667 val_loss: 0.52377 val_acc: 0.88000\n",
            "Epoch [560/10000], loss: 0.53233 acc: 0.90667 val_loss: 0.52110 val_acc: 0.88000\n",
            "Epoch [570/10000], loss: 0.52981 acc: 0.90667 val_loss: 0.51848 val_acc: 0.88000\n",
            "Epoch [580/10000], loss: 0.52734 acc: 0.90667 val_loss: 0.51592 val_acc: 0.88000\n",
            "Epoch [590/10000], loss: 0.52493 acc: 0.90667 val_loss: 0.51342 val_acc: 0.88000\n",
            "Epoch [600/10000], loss: 0.52256 acc: 0.90667 val_loss: 0.51098 val_acc: 0.88000\n",
            "Epoch [610/10000], loss: 0.52025 acc: 0.90667 val_loss: 0.50859 val_acc: 0.88000\n",
            "Epoch [620/10000], loss: 0.51798 acc: 0.90667 val_loss: 0.50624 val_acc: 0.88000\n",
            "Epoch [630/10000], loss: 0.51576 acc: 0.90667 val_loss: 0.50395 val_acc: 0.88000\n",
            "Epoch [640/10000], loss: 0.51358 acc: 0.90667 val_loss: 0.50170 val_acc: 0.88000\n",
            "Epoch [650/10000], loss: 0.51144 acc: 0.90667 val_loss: 0.49949 val_acc: 0.88000\n",
            "Epoch [660/10000], loss: 0.50934 acc: 0.90667 val_loss: 0.49733 val_acc: 0.89333\n",
            "Epoch [670/10000], loss: 0.50728 acc: 0.90667 val_loss: 0.49521 val_acc: 0.90667\n",
            "Epoch [680/10000], loss: 0.50526 acc: 0.90667 val_loss: 0.49313 val_acc: 0.90667\n",
            "Epoch [690/10000], loss: 0.50328 acc: 0.90667 val_loss: 0.49109 val_acc: 0.90667\n",
            "Epoch [700/10000], loss: 0.50133 acc: 0.90667 val_loss: 0.48908 val_acc: 0.90667\n",
            "Epoch [710/10000], loss: 0.49941 acc: 0.90667 val_loss: 0.48711 val_acc: 0.90667\n",
            "Epoch [720/10000], loss: 0.49752 acc: 0.90667 val_loss: 0.48517 val_acc: 0.90667\n",
            "Epoch [730/10000], loss: 0.49567 acc: 0.90667 val_loss: 0.48327 val_acc: 0.90667\n",
            "Epoch [740/10000], loss: 0.49385 acc: 0.90667 val_loss: 0.48140 val_acc: 0.90667\n",
            "Epoch [750/10000], loss: 0.49205 acc: 0.90667 val_loss: 0.47956 val_acc: 0.90667\n",
            "Epoch [760/10000], loss: 0.49029 acc: 0.90667 val_loss: 0.47775 val_acc: 0.90667\n",
            "Epoch [770/10000], loss: 0.48855 acc: 0.90667 val_loss: 0.47597 val_acc: 0.90667\n",
            "Epoch [780/10000], loss: 0.48684 acc: 0.90667 val_loss: 0.47422 val_acc: 0.90667\n",
            "Epoch [790/10000], loss: 0.48515 acc: 0.89333 val_loss: 0.47249 val_acc: 0.92000\n",
            "Epoch [800/10000], loss: 0.48349 acc: 0.89333 val_loss: 0.47079 val_acc: 0.92000\n",
            "Epoch [810/10000], loss: 0.48186 acc: 0.89333 val_loss: 0.46912 val_acc: 0.92000\n",
            "Epoch [820/10000], loss: 0.48024 acc: 0.89333 val_loss: 0.46747 val_acc: 0.92000\n",
            "Epoch [830/10000], loss: 0.47865 acc: 0.89333 val_loss: 0.46585 val_acc: 0.92000\n",
            "Epoch [840/10000], loss: 0.47709 acc: 0.89333 val_loss: 0.46425 val_acc: 0.92000\n",
            "Epoch [850/10000], loss: 0.47554 acc: 0.89333 val_loss: 0.46267 val_acc: 0.92000\n",
            "Epoch [860/10000], loss: 0.47402 acc: 0.89333 val_loss: 0.46111 val_acc: 0.92000\n",
            "Epoch [870/10000], loss: 0.47251 acc: 0.89333 val_loss: 0.45958 val_acc: 0.92000\n",
            "Epoch [880/10000], loss: 0.47103 acc: 0.89333 val_loss: 0.45806 val_acc: 0.92000\n",
            "Epoch [890/10000], loss: 0.46956 acc: 0.89333 val_loss: 0.45657 val_acc: 0.92000\n",
            "Epoch [900/10000], loss: 0.46811 acc: 0.89333 val_loss: 0.45509 val_acc: 0.92000\n",
            "Epoch [910/10000], loss: 0.46668 acc: 0.89333 val_loss: 0.45364 val_acc: 0.92000\n",
            "Epoch [920/10000], loss: 0.46527 acc: 0.89333 val_loss: 0.45220 val_acc: 0.92000\n",
            "Epoch [930/10000], loss: 0.46388 acc: 0.89333 val_loss: 0.45078 val_acc: 0.92000\n",
            "Epoch [940/10000], loss: 0.46250 acc: 0.89333 val_loss: 0.44938 val_acc: 0.92000\n",
            "Epoch [950/10000], loss: 0.46114 acc: 0.89333 val_loss: 0.44800 val_acc: 0.92000\n",
            "Epoch [960/10000], loss: 0.45980 acc: 0.89333 val_loss: 0.44663 val_acc: 0.92000\n",
            "Epoch [970/10000], loss: 0.45847 acc: 0.89333 val_loss: 0.44528 val_acc: 0.92000\n",
            "Epoch [980/10000], loss: 0.45716 acc: 0.89333 val_loss: 0.44395 val_acc: 0.92000\n",
            "Epoch [990/10000], loss: 0.45586 acc: 0.89333 val_loss: 0.44263 val_acc: 0.92000\n",
            "Epoch [1000/10000], loss: 0.45458 acc: 0.89333 val_loss: 0.44133 val_acc: 0.92000\n",
            "Epoch [1010/10000], loss: 0.45331 acc: 0.89333 val_loss: 0.44004 val_acc: 0.92000\n",
            "Epoch [1020/10000], loss: 0.45205 acc: 0.89333 val_loss: 0.43877 val_acc: 0.92000\n",
            "Epoch [1030/10000], loss: 0.45081 acc: 0.89333 val_loss: 0.43751 val_acc: 0.92000\n",
            "Epoch [1040/10000], loss: 0.44958 acc: 0.89333 val_loss: 0.43626 val_acc: 0.92000\n",
            "Epoch [1050/10000], loss: 0.44836 acc: 0.89333 val_loss: 0.43503 val_acc: 0.92000\n",
            "Epoch [1060/10000], loss: 0.44716 acc: 0.89333 val_loss: 0.43381 val_acc: 0.92000\n",
            "Epoch [1070/10000], loss: 0.44597 acc: 0.89333 val_loss: 0.43260 val_acc: 0.92000\n",
            "Epoch [1080/10000], loss: 0.44479 acc: 0.89333 val_loss: 0.43141 val_acc: 0.92000\n",
            "Epoch [1090/10000], loss: 0.44363 acc: 0.89333 val_loss: 0.43023 val_acc: 0.92000\n",
            "Epoch [1100/10000], loss: 0.44247 acc: 0.89333 val_loss: 0.42906 val_acc: 0.92000\n",
            "Epoch [1110/10000], loss: 0.44133 acc: 0.89333 val_loss: 0.42790 val_acc: 0.92000\n",
            "Epoch [1120/10000], loss: 0.44020 acc: 0.89333 val_loss: 0.42676 val_acc: 0.92000\n",
            "Epoch [1130/10000], loss: 0.43908 acc: 0.89333 val_loss: 0.42562 val_acc: 0.92000\n",
            "Epoch [1140/10000], loss: 0.43797 acc: 0.89333 val_loss: 0.42450 val_acc: 0.92000\n",
            "Epoch [1150/10000], loss: 0.43687 acc: 0.89333 val_loss: 0.42339 val_acc: 0.92000\n",
            "Epoch [1160/10000], loss: 0.43578 acc: 0.89333 val_loss: 0.42229 val_acc: 0.92000\n",
            "Epoch [1170/10000], loss: 0.43470 acc: 0.89333 val_loss: 0.42120 val_acc: 0.92000\n",
            "Epoch [1180/10000], loss: 0.43363 acc: 0.89333 val_loss: 0.42012 val_acc: 0.92000\n",
            "Epoch [1190/10000], loss: 0.43257 acc: 0.89333 val_loss: 0.41905 val_acc: 0.92000\n",
            "Epoch [1200/10000], loss: 0.43152 acc: 0.89333 val_loss: 0.41799 val_acc: 0.92000\n",
            "Epoch [1210/10000], loss: 0.43048 acc: 0.89333 val_loss: 0.41694 val_acc: 0.92000\n",
            "Epoch [1220/10000], loss: 0.42945 acc: 0.89333 val_loss: 0.41590 val_acc: 0.92000\n",
            "Epoch [1230/10000], loss: 0.42843 acc: 0.89333 val_loss: 0.41487 val_acc: 0.92000\n",
            "Epoch [1240/10000], loss: 0.42742 acc: 0.89333 val_loss: 0.41384 val_acc: 0.92000\n",
            "Epoch [1250/10000], loss: 0.42641 acc: 0.89333 val_loss: 0.41283 val_acc: 0.92000\n",
            "Epoch [1260/10000], loss: 0.42542 acc: 0.89333 val_loss: 0.41182 val_acc: 0.92000\n",
            "Epoch [1270/10000], loss: 0.42443 acc: 0.89333 val_loss: 0.41083 val_acc: 0.92000\n",
            "Epoch [1280/10000], loss: 0.42345 acc: 0.89333 val_loss: 0.40984 val_acc: 0.92000\n",
            "Epoch [1290/10000], loss: 0.42248 acc: 0.89333 val_loss: 0.40886 val_acc: 0.92000\n",
            "Epoch [1300/10000], loss: 0.42152 acc: 0.89333 val_loss: 0.40789 val_acc: 0.92000\n",
            "Epoch [1310/10000], loss: 0.42056 acc: 0.89333 val_loss: 0.40693 val_acc: 0.92000\n",
            "Epoch [1320/10000], loss: 0.41962 acc: 0.89333 val_loss: 0.40598 val_acc: 0.92000\n",
            "Epoch [1330/10000], loss: 0.41868 acc: 0.89333 val_loss: 0.40503 val_acc: 0.93333\n",
            "Epoch [1340/10000], loss: 0.41775 acc: 0.89333 val_loss: 0.40409 val_acc: 0.93333\n",
            "Epoch [1350/10000], loss: 0.41682 acc: 0.89333 val_loss: 0.40316 val_acc: 0.93333\n",
            "Epoch [1360/10000], loss: 0.41590 acc: 0.89333 val_loss: 0.40224 val_acc: 0.93333\n",
            "Epoch [1370/10000], loss: 0.41499 acc: 0.89333 val_loss: 0.40132 val_acc: 0.93333\n",
            "Epoch [1380/10000], loss: 0.41409 acc: 0.89333 val_loss: 0.40041 val_acc: 0.93333\n",
            "Epoch [1390/10000], loss: 0.41320 acc: 0.89333 val_loss: 0.39951 val_acc: 0.93333\n",
            "Epoch [1400/10000], loss: 0.41231 acc: 0.89333 val_loss: 0.39861 val_acc: 0.93333\n",
            "Epoch [1410/10000], loss: 0.41143 acc: 0.89333 val_loss: 0.39773 val_acc: 0.93333\n",
            "Epoch [1420/10000], loss: 0.41055 acc: 0.89333 val_loss: 0.39685 val_acc: 0.93333\n",
            "Epoch [1430/10000], loss: 0.40968 acc: 0.89333 val_loss: 0.39597 val_acc: 0.93333\n",
            "Epoch [1440/10000], loss: 0.40882 acc: 0.89333 val_loss: 0.39510 val_acc: 0.93333\n",
            "Epoch [1450/10000], loss: 0.40796 acc: 0.89333 val_loss: 0.39424 val_acc: 0.93333\n",
            "Epoch [1460/10000], loss: 0.40711 acc: 0.89333 val_loss: 0.39339 val_acc: 0.93333\n",
            "Epoch [1470/10000], loss: 0.40627 acc: 0.89333 val_loss: 0.39254 val_acc: 0.93333\n",
            "Epoch [1480/10000], loss: 0.40543 acc: 0.90667 val_loss: 0.39170 val_acc: 0.93333\n",
            "Epoch [1490/10000], loss: 0.40460 acc: 0.90667 val_loss: 0.39086 val_acc: 0.93333\n",
            "Epoch [1500/10000], loss: 0.40378 acc: 0.90667 val_loss: 0.39003 val_acc: 0.93333\n",
            "Epoch [1510/10000], loss: 0.40296 acc: 0.90667 val_loss: 0.38921 val_acc: 0.93333\n",
            "Epoch [1520/10000], loss: 0.40214 acc: 0.90667 val_loss: 0.38839 val_acc: 0.93333\n",
            "Epoch [1530/10000], loss: 0.40134 acc: 0.90667 val_loss: 0.38758 val_acc: 0.93333\n",
            "Epoch [1540/10000], loss: 0.40053 acc: 0.90667 val_loss: 0.38677 val_acc: 0.93333\n",
            "Epoch [1550/10000], loss: 0.39974 acc: 0.90667 val_loss: 0.38597 val_acc: 0.93333\n",
            "Epoch [1560/10000], loss: 0.39894 acc: 0.90667 val_loss: 0.38517 val_acc: 0.94667\n",
            "Epoch [1570/10000], loss: 0.39816 acc: 0.90667 val_loss: 0.38438 val_acc: 0.94667\n",
            "Epoch [1580/10000], loss: 0.39738 acc: 0.90667 val_loss: 0.38360 val_acc: 0.94667\n",
            "Epoch [1590/10000], loss: 0.39660 acc: 0.90667 val_loss: 0.38282 val_acc: 0.94667\n",
            "Epoch [1600/10000], loss: 0.39583 acc: 0.90667 val_loss: 0.38204 val_acc: 0.94667\n",
            "Epoch [1610/10000], loss: 0.39507 acc: 0.90667 val_loss: 0.38128 val_acc: 0.94667\n",
            "Epoch [1620/10000], loss: 0.39431 acc: 0.90667 val_loss: 0.38051 val_acc: 0.94667\n",
            "Epoch [1630/10000], loss: 0.39355 acc: 0.90667 val_loss: 0.37975 val_acc: 0.94667\n",
            "Epoch [1640/10000], loss: 0.39280 acc: 0.90667 val_loss: 0.37900 val_acc: 0.94667\n",
            "Epoch [1650/10000], loss: 0.39206 acc: 0.90667 val_loss: 0.37825 val_acc: 0.94667\n",
            "Epoch [1660/10000], loss: 0.39132 acc: 0.90667 val_loss: 0.37751 val_acc: 0.94667\n",
            "Epoch [1670/10000], loss: 0.39058 acc: 0.90667 val_loss: 0.37677 val_acc: 0.94667\n",
            "Epoch [1680/10000], loss: 0.38985 acc: 0.90667 val_loss: 0.37604 val_acc: 0.94667\n",
            "Epoch [1690/10000], loss: 0.38913 acc: 0.90667 val_loss: 0.37531 val_acc: 0.94667\n",
            "Epoch [1700/10000], loss: 0.38841 acc: 0.90667 val_loss: 0.37458 val_acc: 0.94667\n",
            "Epoch [1710/10000], loss: 0.38769 acc: 0.90667 val_loss: 0.37386 val_acc: 0.94667\n",
            "Epoch [1720/10000], loss: 0.38698 acc: 0.90667 val_loss: 0.37315 val_acc: 0.94667\n",
            "Epoch [1730/10000], loss: 0.38627 acc: 0.90667 val_loss: 0.37244 val_acc: 0.94667\n",
            "Epoch [1740/10000], loss: 0.38557 acc: 0.90667 val_loss: 0.37173 val_acc: 0.94667\n",
            "Epoch [1750/10000], loss: 0.38487 acc: 0.90667 val_loss: 0.37103 val_acc: 0.94667\n",
            "Epoch [1760/10000], loss: 0.38417 acc: 0.90667 val_loss: 0.37033 val_acc: 0.94667\n",
            "Epoch [1770/10000], loss: 0.38348 acc: 0.90667 val_loss: 0.36964 val_acc: 0.94667\n",
            "Epoch [1780/10000], loss: 0.38280 acc: 0.90667 val_loss: 0.36895 val_acc: 0.94667\n",
            "Epoch [1790/10000], loss: 0.38212 acc: 0.90667 val_loss: 0.36826 val_acc: 0.94667\n",
            "Epoch [1800/10000], loss: 0.38144 acc: 0.90667 val_loss: 0.36758 val_acc: 0.94667\n",
            "Epoch [1810/10000], loss: 0.38076 acc: 0.90667 val_loss: 0.36690 val_acc: 0.94667\n",
            "Epoch [1820/10000], loss: 0.38009 acc: 0.90667 val_loss: 0.36623 val_acc: 0.94667\n",
            "Epoch [1830/10000], loss: 0.37943 acc: 0.90667 val_loss: 0.36556 val_acc: 0.94667\n",
            "Epoch [1840/10000], loss: 0.37877 acc: 0.90667 val_loss: 0.36490 val_acc: 0.94667\n",
            "Epoch [1850/10000], loss: 0.37811 acc: 0.90667 val_loss: 0.36424 val_acc: 0.94667\n",
            "Epoch [1860/10000], loss: 0.37746 acc: 0.90667 val_loss: 0.36358 val_acc: 0.94667\n",
            "Epoch [1870/10000], loss: 0.37681 acc: 0.90667 val_loss: 0.36293 val_acc: 0.94667\n",
            "Epoch [1880/10000], loss: 0.37616 acc: 0.90667 val_loss: 0.36228 val_acc: 0.94667\n",
            "Epoch [1890/10000], loss: 0.37552 acc: 0.90667 val_loss: 0.36163 val_acc: 0.94667\n",
            "Epoch [1900/10000], loss: 0.37488 acc: 0.90667 val_loss: 0.36099 val_acc: 0.94667\n",
            "Epoch [1910/10000], loss: 0.37424 acc: 0.90667 val_loss: 0.36035 val_acc: 0.94667\n",
            "Epoch [1920/10000], loss: 0.37361 acc: 0.90667 val_loss: 0.35972 val_acc: 0.94667\n",
            "Epoch [1930/10000], loss: 0.37298 acc: 0.90667 val_loss: 0.35909 val_acc: 0.94667\n",
            "Epoch [1940/10000], loss: 0.37236 acc: 0.90667 val_loss: 0.35846 val_acc: 0.94667\n",
            "Epoch [1950/10000], loss: 0.37174 acc: 0.90667 val_loss: 0.35784 val_acc: 0.94667\n",
            "Epoch [1960/10000], loss: 0.37112 acc: 0.90667 val_loss: 0.35722 val_acc: 0.94667\n",
            "Epoch [1970/10000], loss: 0.37051 acc: 0.90667 val_loss: 0.35660 val_acc: 0.94667\n",
            "Epoch [1980/10000], loss: 0.36990 acc: 0.90667 val_loss: 0.35599 val_acc: 0.94667\n",
            "Epoch [1990/10000], loss: 0.36929 acc: 0.90667 val_loss: 0.35538 val_acc: 0.94667\n",
            "Epoch [2000/10000], loss: 0.36869 acc: 0.90667 val_loss: 0.35477 val_acc: 0.94667\n",
            "Epoch [2010/10000], loss: 0.36809 acc: 0.90667 val_loss: 0.35417 val_acc: 0.94667\n",
            "Epoch [2020/10000], loss: 0.36749 acc: 0.90667 val_loss: 0.35357 val_acc: 0.94667\n",
            "Epoch [2030/10000], loss: 0.36690 acc: 0.90667 val_loss: 0.35298 val_acc: 0.94667\n",
            "Epoch [2040/10000], loss: 0.36631 acc: 0.90667 val_loss: 0.35238 val_acc: 0.94667\n",
            "Epoch [2050/10000], loss: 0.36572 acc: 0.90667 val_loss: 0.35179 val_acc: 0.94667\n",
            "Epoch [2060/10000], loss: 0.36514 acc: 0.90667 val_loss: 0.35121 val_acc: 0.94667\n",
            "Epoch [2070/10000], loss: 0.36455 acc: 0.90667 val_loss: 0.35062 val_acc: 0.94667\n",
            "Epoch [2080/10000], loss: 0.36398 acc: 0.90667 val_loss: 0.35004 val_acc: 0.94667\n",
            "Epoch [2090/10000], loss: 0.36340 acc: 0.90667 val_loss: 0.34947 val_acc: 0.94667\n",
            "Epoch [2100/10000], loss: 0.36283 acc: 0.90667 val_loss: 0.34889 val_acc: 0.94667\n",
            "Epoch [2110/10000], loss: 0.36226 acc: 0.90667 val_loss: 0.34832 val_acc: 0.94667\n",
            "Epoch [2120/10000], loss: 0.36170 acc: 0.90667 val_loss: 0.34775 val_acc: 0.94667\n",
            "Epoch [2130/10000], loss: 0.36114 acc: 0.90667 val_loss: 0.34719 val_acc: 0.94667\n",
            "Epoch [2140/10000], loss: 0.36058 acc: 0.90667 val_loss: 0.34663 val_acc: 0.94667\n",
            "Epoch [2150/10000], loss: 0.36002 acc: 0.90667 val_loss: 0.34607 val_acc: 0.94667\n",
            "Epoch [2160/10000], loss: 0.35947 acc: 0.90667 val_loss: 0.34551 val_acc: 0.94667\n",
            "Epoch [2170/10000], loss: 0.35892 acc: 0.90667 val_loss: 0.34496 val_acc: 0.94667\n",
            "Epoch [2180/10000], loss: 0.35837 acc: 0.90667 val_loss: 0.34441 val_acc: 0.94667\n",
            "Epoch [2190/10000], loss: 0.35782 acc: 0.90667 val_loss: 0.34386 val_acc: 0.94667\n",
            "Epoch [2200/10000], loss: 0.35728 acc: 0.90667 val_loss: 0.34331 val_acc: 0.94667\n",
            "Epoch [2210/10000], loss: 0.35674 acc: 0.90667 val_loss: 0.34277 val_acc: 0.94667\n",
            "Epoch [2220/10000], loss: 0.35621 acc: 0.90667 val_loss: 0.34223 val_acc: 0.94667\n",
            "Epoch [2230/10000], loss: 0.35567 acc: 0.90667 val_loss: 0.34170 val_acc: 0.94667\n",
            "Epoch [2240/10000], loss: 0.35514 acc: 0.90667 val_loss: 0.34116 val_acc: 0.94667\n",
            "Epoch [2250/10000], loss: 0.35461 acc: 0.90667 val_loss: 0.34063 val_acc: 0.94667\n",
            "Epoch [2260/10000], loss: 0.35409 acc: 0.90667 val_loss: 0.34010 val_acc: 0.94667\n",
            "Epoch [2270/10000], loss: 0.35356 acc: 0.90667 val_loss: 0.33958 val_acc: 0.94667\n",
            "Epoch [2280/10000], loss: 0.35304 acc: 0.90667 val_loss: 0.33905 val_acc: 0.94667\n",
            "Epoch [2290/10000], loss: 0.35253 acc: 0.90667 val_loss: 0.33853 val_acc: 0.94667\n",
            "Epoch [2300/10000], loss: 0.35201 acc: 0.90667 val_loss: 0.33802 val_acc: 0.94667\n",
            "Epoch [2310/10000], loss: 0.35150 acc: 0.90667 val_loss: 0.33750 val_acc: 0.94667\n",
            "Epoch [2320/10000], loss: 0.35099 acc: 0.90667 val_loss: 0.33699 val_acc: 0.94667\n",
            "Epoch [2330/10000], loss: 0.35048 acc: 0.90667 val_loss: 0.33648 val_acc: 0.94667\n",
            "Epoch [2340/10000], loss: 0.34998 acc: 0.90667 val_loss: 0.33597 val_acc: 0.94667\n",
            "Epoch [2350/10000], loss: 0.34947 acc: 0.90667 val_loss: 0.33546 val_acc: 0.94667\n",
            "Epoch [2360/10000], loss: 0.34897 acc: 0.90667 val_loss: 0.33496 val_acc: 0.94667\n",
            "Epoch [2370/10000], loss: 0.34848 acc: 0.90667 val_loss: 0.33446 val_acc: 0.94667\n",
            "Epoch [2380/10000], loss: 0.34798 acc: 0.90667 val_loss: 0.33396 val_acc: 0.94667\n",
            "Epoch [2390/10000], loss: 0.34749 acc: 0.90667 val_loss: 0.33347 val_acc: 0.94667\n",
            "Epoch [2400/10000], loss: 0.34700 acc: 0.90667 val_loss: 0.33297 val_acc: 0.94667\n",
            "Epoch [2410/10000], loss: 0.34651 acc: 0.90667 val_loss: 0.33248 val_acc: 0.94667\n",
            "Epoch [2420/10000], loss: 0.34602 acc: 0.90667 val_loss: 0.33199 val_acc: 0.94667\n",
            "Epoch [2430/10000], loss: 0.34554 acc: 0.90667 val_loss: 0.33151 val_acc: 0.94667\n",
            "Epoch [2440/10000], loss: 0.34506 acc: 0.90667 val_loss: 0.33102 val_acc: 0.94667\n",
            "Epoch [2450/10000], loss: 0.34458 acc: 0.90667 val_loss: 0.33054 val_acc: 0.94667\n",
            "Epoch [2460/10000], loss: 0.34411 acc: 0.90667 val_loss: 0.33006 val_acc: 0.94667\n",
            "Epoch [2470/10000], loss: 0.34363 acc: 0.90667 val_loss: 0.32959 val_acc: 0.94667\n",
            "Epoch [2480/10000], loss: 0.34316 acc: 0.90667 val_loss: 0.32911 val_acc: 0.94667\n",
            "Epoch [2490/10000], loss: 0.34269 acc: 0.90667 val_loss: 0.32864 val_acc: 0.94667\n",
            "Epoch [2500/10000], loss: 0.34222 acc: 0.90667 val_loss: 0.32817 val_acc: 0.94667\n",
            "Epoch [2510/10000], loss: 0.34176 acc: 0.90667 val_loss: 0.32770 val_acc: 0.94667\n",
            "Epoch [2520/10000], loss: 0.34130 acc: 0.90667 val_loss: 0.32723 val_acc: 0.94667\n",
            "Epoch [2530/10000], loss: 0.34083 acc: 0.90667 val_loss: 0.32677 val_acc: 0.94667\n",
            "Epoch [2540/10000], loss: 0.34038 acc: 0.90667 val_loss: 0.32631 val_acc: 0.94667\n",
            "Epoch [2550/10000], loss: 0.33992 acc: 0.90667 val_loss: 0.32585 val_acc: 0.94667\n",
            "Epoch [2560/10000], loss: 0.33947 acc: 0.90667 val_loss: 0.32539 val_acc: 0.94667\n",
            "Epoch [2570/10000], loss: 0.33901 acc: 0.90667 val_loss: 0.32493 val_acc: 0.94667\n",
            "Epoch [2580/10000], loss: 0.33856 acc: 0.90667 val_loss: 0.32448 val_acc: 0.94667\n",
            "Epoch [2590/10000], loss: 0.33812 acc: 0.90667 val_loss: 0.32403 val_acc: 0.94667\n",
            "Epoch [2600/10000], loss: 0.33767 acc: 0.90667 val_loss: 0.32358 val_acc: 0.94667\n",
            "Epoch [2610/10000], loss: 0.33723 acc: 0.90667 val_loss: 0.32313 val_acc: 0.94667\n",
            "Epoch [2620/10000], loss: 0.33678 acc: 0.90667 val_loss: 0.32269 val_acc: 0.94667\n",
            "Epoch [2630/10000], loss: 0.33634 acc: 0.90667 val_loss: 0.32225 val_acc: 0.94667\n",
            "Epoch [2640/10000], loss: 0.33591 acc: 0.90667 val_loss: 0.32180 val_acc: 0.94667\n",
            "Epoch [2650/10000], loss: 0.33547 acc: 0.90667 val_loss: 0.32136 val_acc: 0.94667\n",
            "Epoch [2660/10000], loss: 0.33504 acc: 0.90667 val_loss: 0.32093 val_acc: 0.94667\n",
            "Epoch [2670/10000], loss: 0.33460 acc: 0.90667 val_loss: 0.32049 val_acc: 0.94667\n",
            "Epoch [2680/10000], loss: 0.33417 acc: 0.90667 val_loss: 0.32006 val_acc: 0.94667\n",
            "Epoch [2690/10000], loss: 0.33375 acc: 0.90667 val_loss: 0.31963 val_acc: 0.94667\n",
            "Epoch [2700/10000], loss: 0.33332 acc: 0.90667 val_loss: 0.31920 val_acc: 0.94667\n",
            "Epoch [2710/10000], loss: 0.33290 acc: 0.90667 val_loss: 0.31877 val_acc: 0.94667\n",
            "Epoch [2720/10000], loss: 0.33247 acc: 0.90667 val_loss: 0.31834 val_acc: 0.94667\n",
            "Epoch [2730/10000], loss: 0.33205 acc: 0.90667 val_loss: 0.31792 val_acc: 0.94667\n",
            "Epoch [2740/10000], loss: 0.33164 acc: 0.90667 val_loss: 0.31750 val_acc: 0.94667\n",
            "Epoch [2750/10000], loss: 0.33122 acc: 0.90667 val_loss: 0.31708 val_acc: 0.94667\n",
            "Epoch [2760/10000], loss: 0.33080 acc: 0.90667 val_loss: 0.31666 val_acc: 0.94667\n",
            "Epoch [2770/10000], loss: 0.33039 acc: 0.90667 val_loss: 0.31624 val_acc: 0.94667\n",
            "Epoch [2780/10000], loss: 0.32998 acc: 0.90667 val_loss: 0.31583 val_acc: 0.94667\n",
            "Epoch [2790/10000], loss: 0.32957 acc: 0.90667 val_loss: 0.31542 val_acc: 0.94667\n",
            "Epoch [2800/10000], loss: 0.32916 acc: 0.90667 val_loss: 0.31500 val_acc: 0.94667\n",
            "Epoch [2810/10000], loss: 0.32876 acc: 0.90667 val_loss: 0.31460 val_acc: 0.94667\n",
            "Epoch [2820/10000], loss: 0.32835 acc: 0.90667 val_loss: 0.31419 val_acc: 0.94667\n",
            "Epoch [2830/10000], loss: 0.32795 acc: 0.90667 val_loss: 0.31378 val_acc: 0.94667\n",
            "Epoch [2840/10000], loss: 0.32755 acc: 0.90667 val_loss: 0.31338 val_acc: 0.94667\n",
            "Epoch [2850/10000], loss: 0.32715 acc: 0.90667 val_loss: 0.31297 val_acc: 0.94667\n",
            "Epoch [2860/10000], loss: 0.32675 acc: 0.90667 val_loss: 0.31257 val_acc: 0.94667\n",
            "Epoch [2870/10000], loss: 0.32636 acc: 0.90667 val_loss: 0.31217 val_acc: 0.94667\n",
            "Epoch [2880/10000], loss: 0.32597 acc: 0.90667 val_loss: 0.31178 val_acc: 0.94667\n",
            "Epoch [2890/10000], loss: 0.32557 acc: 0.90667 val_loss: 0.31138 val_acc: 0.94667\n",
            "Epoch [2900/10000], loss: 0.32518 acc: 0.90667 val_loss: 0.31099 val_acc: 0.94667\n",
            "Epoch [2910/10000], loss: 0.32480 acc: 0.90667 val_loss: 0.31060 val_acc: 0.94667\n",
            "Epoch [2920/10000], loss: 0.32441 acc: 0.90667 val_loss: 0.31020 val_acc: 0.94667\n",
            "Epoch [2930/10000], loss: 0.32402 acc: 0.90667 val_loss: 0.30982 val_acc: 0.94667\n",
            "Epoch [2940/10000], loss: 0.32364 acc: 0.90667 val_loss: 0.30943 val_acc: 0.94667\n",
            "Epoch [2950/10000], loss: 0.32326 acc: 0.90667 val_loss: 0.30904 val_acc: 0.94667\n",
            "Epoch [2960/10000], loss: 0.32288 acc: 0.90667 val_loss: 0.30866 val_acc: 0.94667\n",
            "Epoch [2970/10000], loss: 0.32250 acc: 0.90667 val_loss: 0.30827 val_acc: 0.94667\n",
            "Epoch [2980/10000], loss: 0.32212 acc: 0.90667 val_loss: 0.30789 val_acc: 0.94667\n",
            "Epoch [2990/10000], loss: 0.32175 acc: 0.90667 val_loss: 0.30751 val_acc: 0.94667\n",
            "Epoch [3000/10000], loss: 0.32137 acc: 0.90667 val_loss: 0.30714 val_acc: 0.94667\n",
            "Epoch [3010/10000], loss: 0.32100 acc: 0.90667 val_loss: 0.30676 val_acc: 0.94667\n",
            "Epoch [3020/10000], loss: 0.32063 acc: 0.90667 val_loss: 0.30638 val_acc: 0.94667\n",
            "Epoch [3030/10000], loss: 0.32026 acc: 0.90667 val_loss: 0.30601 val_acc: 0.94667\n",
            "Epoch [3040/10000], loss: 0.31989 acc: 0.90667 val_loss: 0.30564 val_acc: 0.94667\n",
            "Epoch [3050/10000], loss: 0.31952 acc: 0.90667 val_loss: 0.30527 val_acc: 0.94667\n",
            "Epoch [3060/10000], loss: 0.31916 acc: 0.90667 val_loss: 0.30490 val_acc: 0.94667\n",
            "Epoch [3070/10000], loss: 0.31880 acc: 0.90667 val_loss: 0.30453 val_acc: 0.94667\n",
            "Epoch [3080/10000], loss: 0.31844 acc: 0.90667 val_loss: 0.30417 val_acc: 0.94667\n",
            "Epoch [3090/10000], loss: 0.31807 acc: 0.90667 val_loss: 0.30380 val_acc: 0.94667\n",
            "Epoch [3100/10000], loss: 0.31772 acc: 0.90667 val_loss: 0.30344 val_acc: 0.94667\n",
            "Epoch [3110/10000], loss: 0.31736 acc: 0.90667 val_loss: 0.30308 val_acc: 0.94667\n",
            "Epoch [3120/10000], loss: 0.31700 acc: 0.90667 val_loss: 0.30272 val_acc: 0.94667\n",
            "Epoch [3130/10000], loss: 0.31665 acc: 0.90667 val_loss: 0.30236 val_acc: 0.94667\n",
            "Epoch [3140/10000], loss: 0.31630 acc: 0.90667 val_loss: 0.30200 val_acc: 0.94667\n",
            "Epoch [3150/10000], loss: 0.31594 acc: 0.90667 val_loss: 0.30165 val_acc: 0.94667\n",
            "Epoch [3160/10000], loss: 0.31559 acc: 0.90667 val_loss: 0.30129 val_acc: 0.94667\n",
            "Epoch [3170/10000], loss: 0.31525 acc: 0.90667 val_loss: 0.30094 val_acc: 0.94667\n",
            "Epoch [3180/10000], loss: 0.31490 acc: 0.90667 val_loss: 0.30059 val_acc: 0.94667\n",
            "Epoch [3190/10000], loss: 0.31455 acc: 0.90667 val_loss: 0.30024 val_acc: 0.94667\n",
            "Epoch [3200/10000], loss: 0.31421 acc: 0.90667 val_loss: 0.29989 val_acc: 0.94667\n",
            "Epoch [3210/10000], loss: 0.31386 acc: 0.90667 val_loss: 0.29954 val_acc: 0.94667\n",
            "Epoch [3220/10000], loss: 0.31352 acc: 0.90667 val_loss: 0.29919 val_acc: 0.94667\n",
            "Epoch [3230/10000], loss: 0.31318 acc: 0.90667 val_loss: 0.29885 val_acc: 0.94667\n",
            "Epoch [3240/10000], loss: 0.31284 acc: 0.90667 val_loss: 0.29851 val_acc: 0.94667\n",
            "Epoch [3250/10000], loss: 0.31251 acc: 0.90667 val_loss: 0.29816 val_acc: 0.94667\n",
            "Epoch [3260/10000], loss: 0.31217 acc: 0.90667 val_loss: 0.29782 val_acc: 0.94667\n",
            "Epoch [3270/10000], loss: 0.31183 acc: 0.90667 val_loss: 0.29748 val_acc: 0.94667\n",
            "Epoch [3280/10000], loss: 0.31150 acc: 0.90667 val_loss: 0.29715 val_acc: 0.94667\n",
            "Epoch [3290/10000], loss: 0.31117 acc: 0.90667 val_loss: 0.29681 val_acc: 0.94667\n",
            "Epoch [3300/10000], loss: 0.31084 acc: 0.90667 val_loss: 0.29647 val_acc: 0.94667\n",
            "Epoch [3310/10000], loss: 0.31051 acc: 0.90667 val_loss: 0.29614 val_acc: 0.94667\n",
            "Epoch [3320/10000], loss: 0.31018 acc: 0.90667 val_loss: 0.29581 val_acc: 0.94667\n",
            "Epoch [3330/10000], loss: 0.30985 acc: 0.90667 val_loss: 0.29548 val_acc: 0.94667\n",
            "Epoch [3340/10000], loss: 0.30953 acc: 0.90667 val_loss: 0.29515 val_acc: 0.94667\n",
            "Epoch [3350/10000], loss: 0.30920 acc: 0.90667 val_loss: 0.29482 val_acc: 0.94667\n",
            "Epoch [3360/10000], loss: 0.30888 acc: 0.90667 val_loss: 0.29449 val_acc: 0.94667\n",
            "Epoch [3370/10000], loss: 0.30856 acc: 0.90667 val_loss: 0.29416 val_acc: 0.94667\n",
            "Epoch [3380/10000], loss: 0.30824 acc: 0.90667 val_loss: 0.29384 val_acc: 0.94667\n",
            "Epoch [3390/10000], loss: 0.30792 acc: 0.90667 val_loss: 0.29351 val_acc: 0.94667\n",
            "Epoch [3400/10000], loss: 0.30760 acc: 0.90667 val_loss: 0.29319 val_acc: 0.94667\n",
            "Epoch [3410/10000], loss: 0.30728 acc: 0.90667 val_loss: 0.29287 val_acc: 0.94667\n",
            "Epoch [3420/10000], loss: 0.30696 acc: 0.90667 val_loss: 0.29255 val_acc: 0.94667\n",
            "Epoch [3430/10000], loss: 0.30665 acc: 0.90667 val_loss: 0.29223 val_acc: 0.94667\n",
            "Epoch [3440/10000], loss: 0.30634 acc: 0.90667 val_loss: 0.29191 val_acc: 0.94667\n",
            "Epoch [3450/10000], loss: 0.30602 acc: 0.90667 val_loss: 0.29159 val_acc: 0.94667\n",
            "Epoch [3460/10000], loss: 0.30571 acc: 0.90667 val_loss: 0.29128 val_acc: 0.94667\n",
            "Epoch [3470/10000], loss: 0.30540 acc: 0.90667 val_loss: 0.29096 val_acc: 0.94667\n",
            "Epoch [3480/10000], loss: 0.30509 acc: 0.90667 val_loss: 0.29065 val_acc: 0.94667\n",
            "Epoch [3490/10000], loss: 0.30479 acc: 0.90667 val_loss: 0.29034 val_acc: 0.94667\n",
            "Epoch [3500/10000], loss: 0.30448 acc: 0.90667 val_loss: 0.29003 val_acc: 0.94667\n",
            "Epoch [3510/10000], loss: 0.30417 acc: 0.90667 val_loss: 0.28972 val_acc: 0.94667\n",
            "Epoch [3520/10000], loss: 0.30387 acc: 0.90667 val_loss: 0.28941 val_acc: 0.94667\n",
            "Epoch [3530/10000], loss: 0.30357 acc: 0.90667 val_loss: 0.28910 val_acc: 0.94667\n",
            "Epoch [3540/10000], loss: 0.30327 acc: 0.90667 val_loss: 0.28879 val_acc: 0.94667\n",
            "Epoch [3550/10000], loss: 0.30297 acc: 0.90667 val_loss: 0.28849 val_acc: 0.94667\n",
            "Epoch [3560/10000], loss: 0.30267 acc: 0.90667 val_loss: 0.28818 val_acc: 0.94667\n",
            "Epoch [3570/10000], loss: 0.30237 acc: 0.90667 val_loss: 0.28788 val_acc: 0.94667\n",
            "Epoch [3580/10000], loss: 0.30207 acc: 0.90667 val_loss: 0.28758 val_acc: 0.94667\n",
            "Epoch [3590/10000], loss: 0.30177 acc: 0.90667 val_loss: 0.28728 val_acc: 0.94667\n",
            "Epoch [3600/10000], loss: 0.30148 acc: 0.90667 val_loss: 0.28698 val_acc: 0.94667\n",
            "Epoch [3610/10000], loss: 0.30119 acc: 0.90667 val_loss: 0.28668 val_acc: 0.94667\n",
            "Epoch [3620/10000], loss: 0.30089 acc: 0.90667 val_loss: 0.28638 val_acc: 0.96000\n",
            "Epoch [3630/10000], loss: 0.30060 acc: 0.90667 val_loss: 0.28608 val_acc: 0.96000\n",
            "Epoch [3640/10000], loss: 0.30031 acc: 0.90667 val_loss: 0.28579 val_acc: 0.96000\n",
            "Epoch [3650/10000], loss: 0.30002 acc: 0.90667 val_loss: 0.28549 val_acc: 0.96000\n",
            "Epoch [3660/10000], loss: 0.29973 acc: 0.90667 val_loss: 0.28520 val_acc: 0.96000\n",
            "Epoch [3670/10000], loss: 0.29944 acc: 0.90667 val_loss: 0.28491 val_acc: 0.96000\n",
            "Epoch [3680/10000], loss: 0.29916 acc: 0.90667 val_loss: 0.28462 val_acc: 0.96000\n",
            "Epoch [3690/10000], loss: 0.29887 acc: 0.90667 val_loss: 0.28433 val_acc: 0.96000\n",
            "Epoch [3700/10000], loss: 0.29859 acc: 0.90667 val_loss: 0.28404 val_acc: 0.96000\n",
            "Epoch [3710/10000], loss: 0.29830 acc: 0.90667 val_loss: 0.28375 val_acc: 0.96000\n",
            "Epoch [3720/10000], loss: 0.29802 acc: 0.90667 val_loss: 0.28346 val_acc: 0.96000\n",
            "Epoch [3730/10000], loss: 0.29774 acc: 0.90667 val_loss: 0.28318 val_acc: 0.96000\n",
            "Epoch [3740/10000], loss: 0.29746 acc: 0.90667 val_loss: 0.28289 val_acc: 0.96000\n",
            "Epoch [3750/10000], loss: 0.29718 acc: 0.90667 val_loss: 0.28261 val_acc: 0.96000\n",
            "Epoch [3760/10000], loss: 0.29690 acc: 0.90667 val_loss: 0.28232 val_acc: 0.96000\n",
            "Epoch [3770/10000], loss: 0.29663 acc: 0.90667 val_loss: 0.28204 val_acc: 0.96000\n",
            "Epoch [3780/10000], loss: 0.29635 acc: 0.90667 val_loss: 0.28176 val_acc: 0.96000\n",
            "Epoch [3790/10000], loss: 0.29607 acc: 0.90667 val_loss: 0.28148 val_acc: 0.96000\n",
            "Epoch [3800/10000], loss: 0.29580 acc: 0.90667 val_loss: 0.28120 val_acc: 0.96000\n",
            "Epoch [3810/10000], loss: 0.29553 acc: 0.90667 val_loss: 0.28092 val_acc: 0.96000\n",
            "Epoch [3820/10000], loss: 0.29525 acc: 0.90667 val_loss: 0.28064 val_acc: 0.96000\n",
            "Epoch [3830/10000], loss: 0.29498 acc: 0.90667 val_loss: 0.28037 val_acc: 0.96000\n",
            "Epoch [3840/10000], loss: 0.29471 acc: 0.90667 val_loss: 0.28009 val_acc: 0.96000\n",
            "Epoch [3850/10000], loss: 0.29444 acc: 0.90667 val_loss: 0.27982 val_acc: 0.96000\n",
            "Epoch [3860/10000], loss: 0.29418 acc: 0.90667 val_loss: 0.27954 val_acc: 0.96000\n",
            "Epoch [3870/10000], loss: 0.29391 acc: 0.90667 val_loss: 0.27927 val_acc: 0.96000\n",
            "Epoch [3880/10000], loss: 0.29364 acc: 0.90667 val_loss: 0.27900 val_acc: 0.96000\n",
            "Epoch [3890/10000], loss: 0.29338 acc: 0.90667 val_loss: 0.27873 val_acc: 0.96000\n",
            "Epoch [3900/10000], loss: 0.29311 acc: 0.90667 val_loss: 0.27846 val_acc: 0.96000\n",
            "Epoch [3910/10000], loss: 0.29285 acc: 0.90667 val_loss: 0.27819 val_acc: 0.96000\n",
            "Epoch [3920/10000], loss: 0.29258 acc: 0.90667 val_loss: 0.27792 val_acc: 0.96000\n",
            "Epoch [3930/10000], loss: 0.29232 acc: 0.90667 val_loss: 0.27766 val_acc: 0.96000\n",
            "Epoch [3940/10000], loss: 0.29206 acc: 0.90667 val_loss: 0.27739 val_acc: 0.96000\n",
            "Epoch [3950/10000], loss: 0.29180 acc: 0.90667 val_loss: 0.27712 val_acc: 0.96000\n",
            "Epoch [3960/10000], loss: 0.29154 acc: 0.90667 val_loss: 0.27686 val_acc: 0.96000\n",
            "Epoch [3970/10000], loss: 0.29128 acc: 0.90667 val_loss: 0.27660 val_acc: 0.96000\n",
            "Epoch [3980/10000], loss: 0.29103 acc: 0.90667 val_loss: 0.27633 val_acc: 0.96000\n",
            "Epoch [3990/10000], loss: 0.29077 acc: 0.90667 val_loss: 0.27607 val_acc: 0.96000\n",
            "Epoch [4000/10000], loss: 0.29052 acc: 0.90667 val_loss: 0.27581 val_acc: 0.96000\n",
            "Epoch [4010/10000], loss: 0.29026 acc: 0.90667 val_loss: 0.27555 val_acc: 0.96000\n",
            "Epoch [4020/10000], loss: 0.29001 acc: 0.90667 val_loss: 0.27529 val_acc: 0.96000\n",
            "Epoch [4030/10000], loss: 0.28975 acc: 0.90667 val_loss: 0.27504 val_acc: 0.96000\n",
            "Epoch [4040/10000], loss: 0.28950 acc: 0.90667 val_loss: 0.27478 val_acc: 0.96000\n",
            "Epoch [4050/10000], loss: 0.28925 acc: 0.90667 val_loss: 0.27452 val_acc: 0.96000\n",
            "Epoch [4060/10000], loss: 0.28900 acc: 0.90667 val_loss: 0.27427 val_acc: 0.96000\n",
            "Epoch [4070/10000], loss: 0.28875 acc: 0.90667 val_loss: 0.27401 val_acc: 0.96000\n",
            "Epoch [4080/10000], loss: 0.28850 acc: 0.90667 val_loss: 0.27376 val_acc: 0.96000\n",
            "Epoch [4090/10000], loss: 0.28826 acc: 0.90667 val_loss: 0.27351 val_acc: 0.96000\n",
            "Epoch [4100/10000], loss: 0.28801 acc: 0.90667 val_loss: 0.27325 val_acc: 0.96000\n",
            "Epoch [4110/10000], loss: 0.28776 acc: 0.90667 val_loss: 0.27300 val_acc: 0.96000\n",
            "Epoch [4120/10000], loss: 0.28752 acc: 0.90667 val_loss: 0.27275 val_acc: 0.96000\n",
            "Epoch [4130/10000], loss: 0.28727 acc: 0.90667 val_loss: 0.27250 val_acc: 0.96000\n",
            "Epoch [4140/10000], loss: 0.28703 acc: 0.90667 val_loss: 0.27225 val_acc: 0.96000\n",
            "Epoch [4150/10000], loss: 0.28679 acc: 0.90667 val_loss: 0.27200 val_acc: 0.96000\n",
            "Epoch [4160/10000], loss: 0.28654 acc: 0.90667 val_loss: 0.27176 val_acc: 0.96000\n",
            "Epoch [4170/10000], loss: 0.28630 acc: 0.90667 val_loss: 0.27151 val_acc: 0.96000\n",
            "Epoch [4180/10000], loss: 0.28606 acc: 0.90667 val_loss: 0.27127 val_acc: 0.96000\n",
            "Epoch [4190/10000], loss: 0.28582 acc: 0.90667 val_loss: 0.27102 val_acc: 0.96000\n",
            "Epoch [4200/10000], loss: 0.28559 acc: 0.90667 val_loss: 0.27078 val_acc: 0.96000\n",
            "Epoch [4210/10000], loss: 0.28535 acc: 0.90667 val_loss: 0.27053 val_acc: 0.96000\n",
            "Epoch [4220/10000], loss: 0.28511 acc: 0.90667 val_loss: 0.27029 val_acc: 0.96000\n",
            "Epoch [4230/10000], loss: 0.28487 acc: 0.90667 val_loss: 0.27005 val_acc: 0.96000\n",
            "Epoch [4240/10000], loss: 0.28464 acc: 0.90667 val_loss: 0.26981 val_acc: 0.96000\n",
            "Epoch [4250/10000], loss: 0.28440 acc: 0.90667 val_loss: 0.26957 val_acc: 0.96000\n",
            "Epoch [4260/10000], loss: 0.28417 acc: 0.90667 val_loss: 0.26933 val_acc: 0.96000\n",
            "Epoch [4270/10000], loss: 0.28394 acc: 0.90667 val_loss: 0.26909 val_acc: 0.96000\n",
            "Epoch [4280/10000], loss: 0.28370 acc: 0.90667 val_loss: 0.26885 val_acc: 0.96000\n",
            "Epoch [4290/10000], loss: 0.28347 acc: 0.90667 val_loss: 0.26862 val_acc: 0.96000\n",
            "Epoch [4300/10000], loss: 0.28324 acc: 0.90667 val_loss: 0.26838 val_acc: 0.96000\n",
            "Epoch [4310/10000], loss: 0.28301 acc: 0.90667 val_loss: 0.26815 val_acc: 0.96000\n",
            "Epoch [4320/10000], loss: 0.28278 acc: 0.90667 val_loss: 0.26791 val_acc: 0.96000\n",
            "Epoch [4330/10000], loss: 0.28255 acc: 0.90667 val_loss: 0.26768 val_acc: 0.96000\n",
            "Epoch [4340/10000], loss: 0.28233 acc: 0.90667 val_loss: 0.26744 val_acc: 0.96000\n",
            "Epoch [4350/10000], loss: 0.28210 acc: 0.90667 val_loss: 0.26721 val_acc: 0.96000\n",
            "Epoch [4360/10000], loss: 0.28187 acc: 0.90667 val_loss: 0.26698 val_acc: 0.96000\n",
            "Epoch [4370/10000], loss: 0.28165 acc: 0.90667 val_loss: 0.26675 val_acc: 0.96000\n",
            "Epoch [4380/10000], loss: 0.28142 acc: 0.90667 val_loss: 0.26652 val_acc: 0.96000\n",
            "Epoch [4390/10000], loss: 0.28120 acc: 0.90667 val_loss: 0.26629 val_acc: 0.96000\n",
            "Epoch [4400/10000], loss: 0.28098 acc: 0.90667 val_loss: 0.26606 val_acc: 0.96000\n",
            "Epoch [4410/10000], loss: 0.28075 acc: 0.90667 val_loss: 0.26583 val_acc: 0.96000\n",
            "Epoch [4420/10000], loss: 0.28053 acc: 0.90667 val_loss: 0.26560 val_acc: 0.96000\n",
            "Epoch [4430/10000], loss: 0.28031 acc: 0.90667 val_loss: 0.26538 val_acc: 0.96000\n",
            "Epoch [4440/10000], loss: 0.28009 acc: 0.90667 val_loss: 0.26515 val_acc: 0.96000\n",
            "Epoch [4450/10000], loss: 0.27987 acc: 0.90667 val_loss: 0.26493 val_acc: 0.96000\n",
            "Epoch [4460/10000], loss: 0.27965 acc: 0.90667 val_loss: 0.26470 val_acc: 0.96000\n",
            "Epoch [4470/10000], loss: 0.27943 acc: 0.90667 val_loss: 0.26448 val_acc: 0.96000\n",
            "Epoch [4480/10000], loss: 0.27922 acc: 0.90667 val_loss: 0.26425 val_acc: 0.96000\n",
            "Epoch [4490/10000], loss: 0.27900 acc: 0.90667 val_loss: 0.26403 val_acc: 0.96000\n",
            "Epoch [4500/10000], loss: 0.27878 acc: 0.90667 val_loss: 0.26381 val_acc: 0.96000\n",
            "Epoch [4510/10000], loss: 0.27857 acc: 0.90667 val_loss: 0.26359 val_acc: 0.96000\n",
            "Epoch [4520/10000], loss: 0.27835 acc: 0.90667 val_loss: 0.26337 val_acc: 0.96000\n",
            "Epoch [4530/10000], loss: 0.27814 acc: 0.90667 val_loss: 0.26315 val_acc: 0.96000\n",
            "Epoch [4540/10000], loss: 0.27792 acc: 0.90667 val_loss: 0.26293 val_acc: 0.96000\n",
            "Epoch [4550/10000], loss: 0.27771 acc: 0.90667 val_loss: 0.26271 val_acc: 0.96000\n",
            "Epoch [4560/10000], loss: 0.27750 acc: 0.90667 val_loss: 0.26249 val_acc: 0.96000\n",
            "Epoch [4570/10000], loss: 0.27729 acc: 0.90667 val_loss: 0.26228 val_acc: 0.96000\n",
            "Epoch [4580/10000], loss: 0.27708 acc: 0.90667 val_loss: 0.26206 val_acc: 0.96000\n",
            "Epoch [4590/10000], loss: 0.27687 acc: 0.90667 val_loss: 0.26185 val_acc: 0.96000\n",
            "Epoch [4600/10000], loss: 0.27666 acc: 0.90667 val_loss: 0.26163 val_acc: 0.96000\n",
            "Epoch [4610/10000], loss: 0.27645 acc: 0.90667 val_loss: 0.26142 val_acc: 0.96000\n",
            "Epoch [4620/10000], loss: 0.27624 acc: 0.90667 val_loss: 0.26120 val_acc: 0.96000\n",
            "Epoch [4630/10000], loss: 0.27603 acc: 0.90667 val_loss: 0.26099 val_acc: 0.96000\n",
            "Epoch [4640/10000], loss: 0.27583 acc: 0.90667 val_loss: 0.26078 val_acc: 0.96000\n",
            "Epoch [4650/10000], loss: 0.27562 acc: 0.90667 val_loss: 0.26057 val_acc: 0.96000\n",
            "Epoch [4660/10000], loss: 0.27541 acc: 0.90667 val_loss: 0.26035 val_acc: 0.96000\n",
            "Epoch [4670/10000], loss: 0.27521 acc: 0.90667 val_loss: 0.26014 val_acc: 0.96000\n",
            "Epoch [4680/10000], loss: 0.27500 acc: 0.90667 val_loss: 0.25993 val_acc: 0.96000\n",
            "Epoch [4690/10000], loss: 0.27480 acc: 0.90667 val_loss: 0.25973 val_acc: 0.96000\n",
            "Epoch [4700/10000], loss: 0.27460 acc: 0.90667 val_loss: 0.25952 val_acc: 0.96000\n",
            "Epoch [4710/10000], loss: 0.27440 acc: 0.90667 val_loss: 0.25931 val_acc: 0.96000\n",
            "Epoch [4720/10000], loss: 0.27419 acc: 0.90667 val_loss: 0.25910 val_acc: 0.96000\n",
            "Epoch [4730/10000], loss: 0.27399 acc: 0.90667 val_loss: 0.25889 val_acc: 0.96000\n",
            "Epoch [4740/10000], loss: 0.27379 acc: 0.90667 val_loss: 0.25869 val_acc: 0.96000\n",
            "Epoch [4750/10000], loss: 0.27359 acc: 0.90667 val_loss: 0.25848 val_acc: 0.96000\n",
            "Epoch [4760/10000], loss: 0.27339 acc: 0.90667 val_loss: 0.25828 val_acc: 0.96000\n",
            "Epoch [4770/10000], loss: 0.27319 acc: 0.90667 val_loss: 0.25807 val_acc: 0.96000\n",
            "Epoch [4780/10000], loss: 0.27300 acc: 0.90667 val_loss: 0.25787 val_acc: 0.96000\n",
            "Epoch [4790/10000], loss: 0.27280 acc: 0.90667 val_loss: 0.25767 val_acc: 0.96000\n",
            "Epoch [4800/10000], loss: 0.27260 acc: 0.90667 val_loss: 0.25746 val_acc: 0.96000\n",
            "Epoch [4810/10000], loss: 0.27241 acc: 0.90667 val_loss: 0.25726 val_acc: 0.96000\n",
            "Epoch [4820/10000], loss: 0.27221 acc: 0.90667 val_loss: 0.25706 val_acc: 0.96000\n",
            "Epoch [4830/10000], loss: 0.27202 acc: 0.90667 val_loss: 0.25686 val_acc: 0.96000\n",
            "Epoch [4840/10000], loss: 0.27182 acc: 0.90667 val_loss: 0.25666 val_acc: 0.96000\n",
            "Epoch [4850/10000], loss: 0.27163 acc: 0.90667 val_loss: 0.25646 val_acc: 0.96000\n",
            "Epoch [4860/10000], loss: 0.27143 acc: 0.90667 val_loss: 0.25626 val_acc: 0.96000\n",
            "Epoch [4870/10000], loss: 0.27124 acc: 0.90667 val_loss: 0.25606 val_acc: 0.96000\n",
            "Epoch [4880/10000], loss: 0.27105 acc: 0.90667 val_loss: 0.25587 val_acc: 0.96000\n",
            "Epoch [4890/10000], loss: 0.27086 acc: 0.90667 val_loss: 0.25567 val_acc: 0.96000\n",
            "Epoch [4900/10000], loss: 0.27067 acc: 0.90667 val_loss: 0.25547 val_acc: 0.96000\n",
            "Epoch [4910/10000], loss: 0.27048 acc: 0.90667 val_loss: 0.25528 val_acc: 0.96000\n",
            "Epoch [4920/10000], loss: 0.27029 acc: 0.90667 val_loss: 0.25508 val_acc: 0.96000\n",
            "Epoch [4930/10000], loss: 0.27010 acc: 0.90667 val_loss: 0.25489 val_acc: 0.96000\n",
            "Epoch [4940/10000], loss: 0.26991 acc: 0.90667 val_loss: 0.25469 val_acc: 0.96000\n",
            "Epoch [4950/10000], loss: 0.26972 acc: 0.90667 val_loss: 0.25450 val_acc: 0.96000\n",
            "Epoch [4960/10000], loss: 0.26953 acc: 0.90667 val_loss: 0.25431 val_acc: 0.96000\n",
            "Epoch [4970/10000], loss: 0.26935 acc: 0.90667 val_loss: 0.25411 val_acc: 0.96000\n",
            "Epoch [4980/10000], loss: 0.26916 acc: 0.90667 val_loss: 0.25392 val_acc: 0.96000\n",
            "Epoch [4990/10000], loss: 0.26897 acc: 0.90667 val_loss: 0.25373 val_acc: 0.96000\n",
            "Epoch [5000/10000], loss: 0.26879 acc: 0.90667 val_loss: 0.25354 val_acc: 0.96000\n",
            "Epoch [5010/10000], loss: 0.26860 acc: 0.90667 val_loss: 0.25335 val_acc: 0.96000\n",
            "Epoch [5020/10000], loss: 0.26842 acc: 0.90667 val_loss: 0.25316 val_acc: 0.96000\n",
            "Epoch [5030/10000], loss: 0.26824 acc: 0.90667 val_loss: 0.25297 val_acc: 0.96000\n",
            "Epoch [5040/10000], loss: 0.26805 acc: 0.90667 val_loss: 0.25278 val_acc: 0.96000\n",
            "Epoch [5050/10000], loss: 0.26787 acc: 0.90667 val_loss: 0.25259 val_acc: 0.96000\n",
            "Epoch [5060/10000], loss: 0.26769 acc: 0.90667 val_loss: 0.25240 val_acc: 0.96000\n",
            "Epoch [5070/10000], loss: 0.26751 acc: 0.90667 val_loss: 0.25222 val_acc: 0.96000\n",
            "Epoch [5080/10000], loss: 0.26733 acc: 0.90667 val_loss: 0.25203 val_acc: 0.96000\n",
            "Epoch [5090/10000], loss: 0.26715 acc: 0.90667 val_loss: 0.25184 val_acc: 0.96000\n",
            "Epoch [5100/10000], loss: 0.26697 acc: 0.90667 val_loss: 0.25166 val_acc: 0.96000\n",
            "Epoch [5110/10000], loss: 0.26679 acc: 0.90667 val_loss: 0.25147 val_acc: 0.96000\n",
            "Epoch [5120/10000], loss: 0.26661 acc: 0.90667 val_loss: 0.25129 val_acc: 0.96000\n",
            "Epoch [5130/10000], loss: 0.26643 acc: 0.90667 val_loss: 0.25111 val_acc: 0.96000\n",
            "Epoch [5140/10000], loss: 0.26625 acc: 0.90667 val_loss: 0.25092 val_acc: 0.96000\n",
            "Epoch [5150/10000], loss: 0.26608 acc: 0.90667 val_loss: 0.25074 val_acc: 0.96000\n",
            "Epoch [5160/10000], loss: 0.26590 acc: 0.90667 val_loss: 0.25056 val_acc: 0.96000\n",
            "Epoch [5170/10000], loss: 0.26572 acc: 0.90667 val_loss: 0.25037 val_acc: 0.96000\n",
            "Epoch [5180/10000], loss: 0.26555 acc: 0.90667 val_loss: 0.25019 val_acc: 0.96000\n",
            "Epoch [5190/10000], loss: 0.26537 acc: 0.90667 val_loss: 0.25001 val_acc: 0.96000\n",
            "Epoch [5200/10000], loss: 0.26520 acc: 0.90667 val_loss: 0.24983 val_acc: 0.96000\n",
            "Epoch [5210/10000], loss: 0.26502 acc: 0.90667 val_loss: 0.24965 val_acc: 0.96000\n",
            "Epoch [5220/10000], loss: 0.26485 acc: 0.90667 val_loss: 0.24947 val_acc: 0.96000\n",
            "Epoch [5230/10000], loss: 0.26468 acc: 0.90667 val_loss: 0.24929 val_acc: 0.96000\n",
            "Epoch [5240/10000], loss: 0.26450 acc: 0.90667 val_loss: 0.24912 val_acc: 0.96000\n",
            "Epoch [5250/10000], loss: 0.26433 acc: 0.90667 val_loss: 0.24894 val_acc: 0.96000\n",
            "Epoch [5260/10000], loss: 0.26416 acc: 0.90667 val_loss: 0.24876 val_acc: 0.96000\n",
            "Epoch [5270/10000], loss: 0.26399 acc: 0.90667 val_loss: 0.24858 val_acc: 0.96000\n",
            "Epoch [5280/10000], loss: 0.26382 acc: 0.90667 val_loss: 0.24841 val_acc: 0.96000\n",
            "Epoch [5290/10000], loss: 0.26365 acc: 0.90667 val_loss: 0.24823 val_acc: 0.96000\n",
            "Epoch [5300/10000], loss: 0.26348 acc: 0.90667 val_loss: 0.24806 val_acc: 0.96000\n",
            "Epoch [5310/10000], loss: 0.26331 acc: 0.90667 val_loss: 0.24788 val_acc: 0.96000\n",
            "Epoch [5320/10000], loss: 0.26314 acc: 0.90667 val_loss: 0.24771 val_acc: 0.96000\n",
            "Epoch [5330/10000], loss: 0.26297 acc: 0.90667 val_loss: 0.24753 val_acc: 0.96000\n",
            "Epoch [5340/10000], loss: 0.26280 acc: 0.90667 val_loss: 0.24736 val_acc: 0.96000\n",
            "Epoch [5350/10000], loss: 0.26264 acc: 0.90667 val_loss: 0.24719 val_acc: 0.96000\n",
            "Epoch [5360/10000], loss: 0.26247 acc: 0.90667 val_loss: 0.24701 val_acc: 0.96000\n",
            "Epoch [5370/10000], loss: 0.26230 acc: 0.90667 val_loss: 0.24684 val_acc: 0.96000\n",
            "Epoch [5380/10000], loss: 0.26214 acc: 0.90667 val_loss: 0.24667 val_acc: 0.96000\n",
            "Epoch [5390/10000], loss: 0.26197 acc: 0.90667 val_loss: 0.24650 val_acc: 0.96000\n",
            "Epoch [5400/10000], loss: 0.26181 acc: 0.90667 val_loss: 0.24633 val_acc: 0.96000\n",
            "Epoch [5410/10000], loss: 0.26164 acc: 0.90667 val_loss: 0.24616 val_acc: 0.96000\n",
            "Epoch [5420/10000], loss: 0.26148 acc: 0.90667 val_loss: 0.24599 val_acc: 0.96000\n",
            "Epoch [5430/10000], loss: 0.26131 acc: 0.90667 val_loss: 0.24582 val_acc: 0.96000\n",
            "Epoch [5440/10000], loss: 0.26115 acc: 0.90667 val_loss: 0.24565 val_acc: 0.96000\n",
            "Epoch [5450/10000], loss: 0.26099 acc: 0.90667 val_loss: 0.24548 val_acc: 0.96000\n",
            "Epoch [5460/10000], loss: 0.26083 acc: 0.90667 val_loss: 0.24531 val_acc: 0.96000\n",
            "Epoch [5470/10000], loss: 0.26066 acc: 0.90667 val_loss: 0.24514 val_acc: 0.96000\n",
            "Epoch [5480/10000], loss: 0.26050 acc: 0.90667 val_loss: 0.24498 val_acc: 0.96000\n",
            "Epoch [5490/10000], loss: 0.26034 acc: 0.90667 val_loss: 0.24481 val_acc: 0.96000\n",
            "Epoch [5500/10000], loss: 0.26018 acc: 0.90667 val_loss: 0.24464 val_acc: 0.96000\n",
            "Epoch [5510/10000], loss: 0.26002 acc: 0.90667 val_loss: 0.24448 val_acc: 0.96000\n",
            "Epoch [5520/10000], loss: 0.25986 acc: 0.90667 val_loss: 0.24431 val_acc: 0.96000\n",
            "Epoch [5530/10000], loss: 0.25970 acc: 0.90667 val_loss: 0.24415 val_acc: 0.96000\n",
            "Epoch [5540/10000], loss: 0.25954 acc: 0.90667 val_loss: 0.24398 val_acc: 0.96000\n",
            "Epoch [5550/10000], loss: 0.25939 acc: 0.90667 val_loss: 0.24382 val_acc: 0.96000\n",
            "Epoch [5560/10000], loss: 0.25923 acc: 0.90667 val_loss: 0.24366 val_acc: 0.96000\n",
            "Epoch [5570/10000], loss: 0.25907 acc: 0.90667 val_loss: 0.24349 val_acc: 0.96000\n",
            "Epoch [5580/10000], loss: 0.25891 acc: 0.90667 val_loss: 0.24333 val_acc: 0.96000\n",
            "Epoch [5590/10000], loss: 0.25876 acc: 0.90667 val_loss: 0.24317 val_acc: 0.96000\n",
            "Epoch [5600/10000], loss: 0.25860 acc: 0.90667 val_loss: 0.24301 val_acc: 0.96000\n",
            "Epoch [5610/10000], loss: 0.25845 acc: 0.90667 val_loss: 0.24285 val_acc: 0.96000\n",
            "Epoch [5620/10000], loss: 0.25829 acc: 0.90667 val_loss: 0.24268 val_acc: 0.96000\n",
            "Epoch [5630/10000], loss: 0.25814 acc: 0.90667 val_loss: 0.24252 val_acc: 0.96000\n",
            "Epoch [5640/10000], loss: 0.25798 acc: 0.90667 val_loss: 0.24236 val_acc: 0.96000\n",
            "Epoch [5650/10000], loss: 0.25783 acc: 0.90667 val_loss: 0.24220 val_acc: 0.96000\n",
            "Epoch [5660/10000], loss: 0.25767 acc: 0.90667 val_loss: 0.24205 val_acc: 0.96000\n",
            "Epoch [5670/10000], loss: 0.25752 acc: 0.90667 val_loss: 0.24189 val_acc: 0.96000\n",
            "Epoch [5680/10000], loss: 0.25737 acc: 0.90667 val_loss: 0.24173 val_acc: 0.96000\n",
            "Epoch [5690/10000], loss: 0.25722 acc: 0.90667 val_loss: 0.24157 val_acc: 0.96000\n",
            "Epoch [5700/10000], loss: 0.25706 acc: 0.90667 val_loss: 0.24141 val_acc: 0.96000\n",
            "Epoch [5710/10000], loss: 0.25691 acc: 0.90667 val_loss: 0.24126 val_acc: 0.96000\n",
            "Epoch [5720/10000], loss: 0.25676 acc: 0.90667 val_loss: 0.24110 val_acc: 0.96000\n",
            "Epoch [5730/10000], loss: 0.25661 acc: 0.90667 val_loss: 0.24094 val_acc: 0.96000\n",
            "Epoch [5740/10000], loss: 0.25646 acc: 0.90667 val_loss: 0.24079 val_acc: 0.96000\n",
            "Epoch [5750/10000], loss: 0.25631 acc: 0.90667 val_loss: 0.24063 val_acc: 0.96000\n",
            "Epoch [5760/10000], loss: 0.25616 acc: 0.90667 val_loss: 0.24048 val_acc: 0.96000\n",
            "Epoch [5770/10000], loss: 0.25601 acc: 0.90667 val_loss: 0.24032 val_acc: 0.96000\n",
            "Epoch [5780/10000], loss: 0.25586 acc: 0.90667 val_loss: 0.24017 val_acc: 0.96000\n",
            "Epoch [5790/10000], loss: 0.25571 acc: 0.90667 val_loss: 0.24001 val_acc: 0.96000\n",
            "Epoch [5800/10000], loss: 0.25557 acc: 0.90667 val_loss: 0.23986 val_acc: 0.96000\n",
            "Epoch [5810/10000], loss: 0.25542 acc: 0.90667 val_loss: 0.23971 val_acc: 0.96000\n",
            "Epoch [5820/10000], loss: 0.25527 acc: 0.90667 val_loss: 0.23955 val_acc: 0.96000\n",
            "Epoch [5830/10000], loss: 0.25513 acc: 0.90667 val_loss: 0.23940 val_acc: 0.96000\n",
            "Epoch [5840/10000], loss: 0.25498 acc: 0.90667 val_loss: 0.23925 val_acc: 0.96000\n",
            "Epoch [5850/10000], loss: 0.25483 acc: 0.90667 val_loss: 0.23910 val_acc: 0.96000\n",
            "Epoch [5860/10000], loss: 0.25469 acc: 0.90667 val_loss: 0.23895 val_acc: 0.96000\n",
            "Epoch [5870/10000], loss: 0.25454 acc: 0.90667 val_loss: 0.23879 val_acc: 0.96000\n",
            "Epoch [5880/10000], loss: 0.25440 acc: 0.90667 val_loss: 0.23864 val_acc: 0.96000\n",
            "Epoch [5890/10000], loss: 0.25425 acc: 0.90667 val_loss: 0.23849 val_acc: 0.96000\n",
            "Epoch [5900/10000], loss: 0.25411 acc: 0.90667 val_loss: 0.23834 val_acc: 0.96000\n",
            "Epoch [5910/10000], loss: 0.25397 acc: 0.90667 val_loss: 0.23819 val_acc: 0.96000\n",
            "Epoch [5920/10000], loss: 0.25382 acc: 0.90667 val_loss: 0.23805 val_acc: 0.96000\n",
            "Epoch [5930/10000], loss: 0.25368 acc: 0.90667 val_loss: 0.23790 val_acc: 0.96000\n",
            "Epoch [5940/10000], loss: 0.25354 acc: 0.90667 val_loss: 0.23775 val_acc: 0.96000\n",
            "Epoch [5950/10000], loss: 0.25340 acc: 0.90667 val_loss: 0.23760 val_acc: 0.96000\n",
            "Epoch [5960/10000], loss: 0.25325 acc: 0.90667 val_loss: 0.23745 val_acc: 0.96000\n",
            "Epoch [5970/10000], loss: 0.25311 acc: 0.90667 val_loss: 0.23731 val_acc: 0.96000\n",
            "Epoch [5980/10000], loss: 0.25297 acc: 0.90667 val_loss: 0.23716 val_acc: 0.96000\n",
            "Epoch [5990/10000], loss: 0.25283 acc: 0.90667 val_loss: 0.23701 val_acc: 0.96000\n",
            "Epoch [6000/10000], loss: 0.25269 acc: 0.90667 val_loss: 0.23687 val_acc: 0.96000\n",
            "Epoch [6010/10000], loss: 0.25255 acc: 0.90667 val_loss: 0.23672 val_acc: 0.96000\n",
            "Epoch [6020/10000], loss: 0.25241 acc: 0.90667 val_loss: 0.23658 val_acc: 0.96000\n",
            "Epoch [6030/10000], loss: 0.25227 acc: 0.90667 val_loss: 0.23643 val_acc: 0.96000\n",
            "Epoch [6040/10000], loss: 0.25213 acc: 0.90667 val_loss: 0.23629 val_acc: 0.96000\n",
            "Epoch [6050/10000], loss: 0.25199 acc: 0.90667 val_loss: 0.23614 val_acc: 0.96000\n",
            "Epoch [6060/10000], loss: 0.25186 acc: 0.90667 val_loss: 0.23600 val_acc: 0.96000\n",
            "Epoch [6070/10000], loss: 0.25172 acc: 0.90667 val_loss: 0.23586 val_acc: 0.96000\n",
            "Epoch [6080/10000], loss: 0.25158 acc: 0.90667 val_loss: 0.23571 val_acc: 0.96000\n",
            "Epoch [6090/10000], loss: 0.25144 acc: 0.90667 val_loss: 0.23557 val_acc: 0.96000\n",
            "Epoch [6100/10000], loss: 0.25131 acc: 0.90667 val_loss: 0.23543 val_acc: 0.96000\n",
            "Epoch [6110/10000], loss: 0.25117 acc: 0.90667 val_loss: 0.23529 val_acc: 0.96000\n",
            "Epoch [6120/10000], loss: 0.25104 acc: 0.90667 val_loss: 0.23514 val_acc: 0.96000\n",
            "Epoch [6130/10000], loss: 0.25090 acc: 0.90667 val_loss: 0.23500 val_acc: 0.96000\n",
            "Epoch [6140/10000], loss: 0.25076 acc: 0.90667 val_loss: 0.23486 val_acc: 0.96000\n",
            "Epoch [6150/10000], loss: 0.25063 acc: 0.90667 val_loss: 0.23472 val_acc: 0.96000\n",
            "Epoch [6160/10000], loss: 0.25049 acc: 0.90667 val_loss: 0.23458 val_acc: 0.96000\n",
            "Epoch [6170/10000], loss: 0.25036 acc: 0.90667 val_loss: 0.23444 val_acc: 0.96000\n",
            "Epoch [6180/10000], loss: 0.25023 acc: 0.90667 val_loss: 0.23430 val_acc: 0.96000\n",
            "Epoch [6190/10000], loss: 0.25009 acc: 0.90667 val_loss: 0.23416 val_acc: 0.96000\n",
            "Epoch [6200/10000], loss: 0.24996 acc: 0.90667 val_loss: 0.23402 val_acc: 0.96000\n",
            "Epoch [6210/10000], loss: 0.24983 acc: 0.90667 val_loss: 0.23388 val_acc: 0.96000\n",
            "Epoch [6220/10000], loss: 0.24969 acc: 0.90667 val_loss: 0.23375 val_acc: 0.96000\n",
            "Epoch [6230/10000], loss: 0.24956 acc: 0.90667 val_loss: 0.23361 val_acc: 0.96000\n",
            "Epoch [6240/10000], loss: 0.24943 acc: 0.90667 val_loss: 0.23347 val_acc: 0.96000\n",
            "Epoch [6250/10000], loss: 0.24930 acc: 0.90667 val_loss: 0.23333 val_acc: 0.96000\n",
            "Epoch [6260/10000], loss: 0.24917 acc: 0.90667 val_loss: 0.23320 val_acc: 0.96000\n",
            "Epoch [6270/10000], loss: 0.24904 acc: 0.90667 val_loss: 0.23306 val_acc: 0.96000\n",
            "Epoch [6280/10000], loss: 0.24891 acc: 0.90667 val_loss: 0.23292 val_acc: 0.96000\n",
            "Epoch [6290/10000], loss: 0.24878 acc: 0.90667 val_loss: 0.23279 val_acc: 0.96000\n",
            "Epoch [6300/10000], loss: 0.24865 acc: 0.90667 val_loss: 0.23265 val_acc: 0.96000\n",
            "Epoch [6310/10000], loss: 0.24852 acc: 0.90667 val_loss: 0.23252 val_acc: 0.96000\n",
            "Epoch [6320/10000], loss: 0.24839 acc: 0.90667 val_loss: 0.23238 val_acc: 0.96000\n",
            "Epoch [6330/10000], loss: 0.24826 acc: 0.90667 val_loss: 0.23225 val_acc: 0.96000\n",
            "Epoch [6340/10000], loss: 0.24813 acc: 0.90667 val_loss: 0.23211 val_acc: 0.96000\n",
            "Epoch [6350/10000], loss: 0.24800 acc: 0.90667 val_loss: 0.23198 val_acc: 0.96000\n",
            "Epoch [6360/10000], loss: 0.24787 acc: 0.90667 val_loss: 0.23184 val_acc: 0.96000\n",
            "Epoch [6370/10000], loss: 0.24775 acc: 0.90667 val_loss: 0.23171 val_acc: 0.96000\n",
            "Epoch [6380/10000], loss: 0.24762 acc: 0.90667 val_loss: 0.23158 val_acc: 0.96000\n",
            "Epoch [6390/10000], loss: 0.24749 acc: 0.90667 val_loss: 0.23145 val_acc: 0.96000\n",
            "Epoch [6400/10000], loss: 0.24736 acc: 0.90667 val_loss: 0.23131 val_acc: 0.96000\n",
            "Epoch [6410/10000], loss: 0.24724 acc: 0.90667 val_loss: 0.23118 val_acc: 0.96000\n",
            "Epoch [6420/10000], loss: 0.24711 acc: 0.90667 val_loss: 0.23105 val_acc: 0.96000\n",
            "Epoch [6430/10000], loss: 0.24699 acc: 0.90667 val_loss: 0.23092 val_acc: 0.96000\n",
            "Epoch [6440/10000], loss: 0.24686 acc: 0.90667 val_loss: 0.23079 val_acc: 0.96000\n",
            "Epoch [6450/10000], loss: 0.24674 acc: 0.90667 val_loss: 0.23066 val_acc: 0.96000\n",
            "Epoch [6460/10000], loss: 0.24661 acc: 0.90667 val_loss: 0.23053 val_acc: 0.96000\n",
            "Epoch [6470/10000], loss: 0.24649 acc: 0.90667 val_loss: 0.23040 val_acc: 0.96000\n",
            "Epoch [6480/10000], loss: 0.24636 acc: 0.90667 val_loss: 0.23027 val_acc: 0.96000\n",
            "Epoch [6490/10000], loss: 0.24624 acc: 0.90667 val_loss: 0.23014 val_acc: 0.96000\n",
            "Epoch [6500/10000], loss: 0.24611 acc: 0.90667 val_loss: 0.23001 val_acc: 0.96000\n",
            "Epoch [6510/10000], loss: 0.24599 acc: 0.90667 val_loss: 0.22988 val_acc: 0.96000\n",
            "Epoch [6520/10000], loss: 0.24587 acc: 0.90667 val_loss: 0.22975 val_acc: 0.96000\n",
            "Epoch [6530/10000], loss: 0.24575 acc: 0.90667 val_loss: 0.22962 val_acc: 0.96000\n",
            "Epoch [6540/10000], loss: 0.24562 acc: 0.90667 val_loss: 0.22949 val_acc: 0.96000\n",
            "Epoch [6550/10000], loss: 0.24550 acc: 0.90667 val_loss: 0.22936 val_acc: 0.96000\n",
            "Epoch [6560/10000], loss: 0.24538 acc: 0.90667 val_loss: 0.22924 val_acc: 0.96000\n",
            "Epoch [6570/10000], loss: 0.24526 acc: 0.90667 val_loss: 0.22911 val_acc: 0.96000\n",
            "Epoch [6580/10000], loss: 0.24514 acc: 0.90667 val_loss: 0.22898 val_acc: 0.96000\n",
            "Epoch [6590/10000], loss: 0.24502 acc: 0.90667 val_loss: 0.22886 val_acc: 0.96000\n",
            "Epoch [6600/10000], loss: 0.24489 acc: 0.90667 val_loss: 0.22873 val_acc: 0.96000\n",
            "Epoch [6610/10000], loss: 0.24477 acc: 0.90667 val_loss: 0.22860 val_acc: 0.96000\n",
            "Epoch [6620/10000], loss: 0.24465 acc: 0.90667 val_loss: 0.22848 val_acc: 0.96000\n",
            "Epoch [6630/10000], loss: 0.24453 acc: 0.90667 val_loss: 0.22835 val_acc: 0.96000\n",
            "Epoch [6640/10000], loss: 0.24441 acc: 0.90667 val_loss: 0.22823 val_acc: 0.96000\n",
            "Epoch [6650/10000], loss: 0.24430 acc: 0.90667 val_loss: 0.22810 val_acc: 0.96000\n",
            "Epoch [6660/10000], loss: 0.24418 acc: 0.90667 val_loss: 0.22798 val_acc: 0.96000\n",
            "Epoch [6670/10000], loss: 0.24406 acc: 0.90667 val_loss: 0.22785 val_acc: 0.96000\n",
            "Epoch [6680/10000], loss: 0.24394 acc: 0.90667 val_loss: 0.22773 val_acc: 0.96000\n",
            "Epoch [6690/10000], loss: 0.24382 acc: 0.90667 val_loss: 0.22761 val_acc: 0.96000\n",
            "Epoch [6700/10000], loss: 0.24370 acc: 0.90667 val_loss: 0.22748 val_acc: 0.96000\n",
            "Epoch [6710/10000], loss: 0.24359 acc: 0.90667 val_loss: 0.22736 val_acc: 0.96000\n",
            "Epoch [6720/10000], loss: 0.24347 acc: 0.90667 val_loss: 0.22724 val_acc: 0.96000\n",
            "Epoch [6730/10000], loss: 0.24335 acc: 0.90667 val_loss: 0.22711 val_acc: 0.96000\n",
            "Epoch [6740/10000], loss: 0.24324 acc: 0.90667 val_loss: 0.22699 val_acc: 0.96000\n",
            "Epoch [6750/10000], loss: 0.24312 acc: 0.90667 val_loss: 0.22687 val_acc: 0.96000\n",
            "Epoch [6760/10000], loss: 0.24300 acc: 0.90667 val_loss: 0.22675 val_acc: 0.96000\n",
            "Epoch [6770/10000], loss: 0.24289 acc: 0.90667 val_loss: 0.22663 val_acc: 0.96000\n",
            "Epoch [6780/10000], loss: 0.24277 acc: 0.90667 val_loss: 0.22651 val_acc: 0.96000\n",
            "Epoch [6790/10000], loss: 0.24266 acc: 0.90667 val_loss: 0.22638 val_acc: 0.96000\n",
            "Epoch [6800/10000], loss: 0.24254 acc: 0.90667 val_loss: 0.22626 val_acc: 0.96000\n",
            "Epoch [6810/10000], loss: 0.24243 acc: 0.90667 val_loss: 0.22614 val_acc: 0.96000\n",
            "Epoch [6820/10000], loss: 0.24231 acc: 0.90667 val_loss: 0.22602 val_acc: 0.96000\n",
            "Epoch [6830/10000], loss: 0.24220 acc: 0.90667 val_loss: 0.22590 val_acc: 0.96000\n",
            "Epoch [6840/10000], loss: 0.24208 acc: 0.90667 val_loss: 0.22578 val_acc: 0.96000\n",
            "Epoch [6850/10000], loss: 0.24197 acc: 0.90667 val_loss: 0.22566 val_acc: 0.96000\n",
            "Epoch [6860/10000], loss: 0.24186 acc: 0.90667 val_loss: 0.22555 val_acc: 0.96000\n",
            "Epoch [6870/10000], loss: 0.24174 acc: 0.90667 val_loss: 0.22543 val_acc: 0.96000\n",
            "Epoch [6880/10000], loss: 0.24163 acc: 0.90667 val_loss: 0.22531 val_acc: 0.96000\n",
            "Epoch [6890/10000], loss: 0.24152 acc: 0.90667 val_loss: 0.22519 val_acc: 0.96000\n",
            "Epoch [6900/10000], loss: 0.24141 acc: 0.90667 val_loss: 0.22507 val_acc: 0.96000\n",
            "Epoch [6910/10000], loss: 0.24129 acc: 0.90667 val_loss: 0.22495 val_acc: 0.96000\n",
            "Epoch [6920/10000], loss: 0.24118 acc: 0.90667 val_loss: 0.22484 val_acc: 0.96000\n",
            "Epoch [6930/10000], loss: 0.24107 acc: 0.90667 val_loss: 0.22472 val_acc: 0.96000\n",
            "Epoch [6940/10000], loss: 0.24096 acc: 0.90667 val_loss: 0.22460 val_acc: 0.96000\n",
            "Epoch [6950/10000], loss: 0.24085 acc: 0.90667 val_loss: 0.22449 val_acc: 0.96000\n",
            "Epoch [6960/10000], loss: 0.24074 acc: 0.90667 val_loss: 0.22437 val_acc: 0.96000\n",
            "Epoch [6970/10000], loss: 0.24063 acc: 0.90667 val_loss: 0.22425 val_acc: 0.96000\n",
            "Epoch [6980/10000], loss: 0.24052 acc: 0.90667 val_loss: 0.22414 val_acc: 0.96000\n",
            "Epoch [6990/10000], loss: 0.24041 acc: 0.90667 val_loss: 0.22402 val_acc: 0.96000\n",
            "Epoch [7000/10000], loss: 0.24030 acc: 0.90667 val_loss: 0.22391 val_acc: 0.96000\n",
            "Epoch [7010/10000], loss: 0.24019 acc: 0.90667 val_loss: 0.22379 val_acc: 0.96000\n",
            "Epoch [7020/10000], loss: 0.24008 acc: 0.90667 val_loss: 0.22368 val_acc: 0.96000\n",
            "Epoch [7030/10000], loss: 0.23997 acc: 0.90667 val_loss: 0.22356 val_acc: 0.96000\n",
            "Epoch [7040/10000], loss: 0.23986 acc: 0.90667 val_loss: 0.22345 val_acc: 0.96000\n",
            "Epoch [7050/10000], loss: 0.23975 acc: 0.90667 val_loss: 0.22333 val_acc: 0.96000\n",
            "Epoch [7060/10000], loss: 0.23964 acc: 0.90667 val_loss: 0.22322 val_acc: 0.96000\n",
            "Epoch [7070/10000], loss: 0.23953 acc: 0.90667 val_loss: 0.22311 val_acc: 0.96000\n",
            "Epoch [7080/10000], loss: 0.23943 acc: 0.90667 val_loss: 0.22299 val_acc: 0.96000\n",
            "Epoch [7090/10000], loss: 0.23932 acc: 0.90667 val_loss: 0.22288 val_acc: 0.96000\n",
            "Epoch [7100/10000], loss: 0.23921 acc: 0.90667 val_loss: 0.22277 val_acc: 0.96000\n",
            "Epoch [7110/10000], loss: 0.23910 acc: 0.90667 val_loss: 0.22265 val_acc: 0.96000\n",
            "Epoch [7120/10000], loss: 0.23900 acc: 0.90667 val_loss: 0.22254 val_acc: 0.96000\n",
            "Epoch [7130/10000], loss: 0.23889 acc: 0.90667 val_loss: 0.22243 val_acc: 0.96000\n",
            "Epoch [7140/10000], loss: 0.23879 acc: 0.90667 val_loss: 0.22232 val_acc: 0.96000\n",
            "Epoch [7150/10000], loss: 0.23868 acc: 0.90667 val_loss: 0.22221 val_acc: 0.96000\n",
            "Epoch [7160/10000], loss: 0.23857 acc: 0.90667 val_loss: 0.22209 val_acc: 0.96000\n",
            "Epoch [7170/10000], loss: 0.23847 acc: 0.90667 val_loss: 0.22198 val_acc: 0.96000\n",
            "Epoch [7180/10000], loss: 0.23836 acc: 0.90667 val_loss: 0.22187 val_acc: 0.96000\n",
            "Epoch [7190/10000], loss: 0.23826 acc: 0.90667 val_loss: 0.22176 val_acc: 0.96000\n",
            "Epoch [7200/10000], loss: 0.23815 acc: 0.90667 val_loss: 0.22165 val_acc: 0.96000\n",
            "Epoch [7210/10000], loss: 0.23805 acc: 0.90667 val_loss: 0.22154 val_acc: 0.96000\n",
            "Epoch [7220/10000], loss: 0.23794 acc: 0.90667 val_loss: 0.22143 val_acc: 0.96000\n",
            "Epoch [7230/10000], loss: 0.23784 acc: 0.90667 val_loss: 0.22132 val_acc: 0.96000\n",
            "Epoch [7240/10000], loss: 0.23774 acc: 0.90667 val_loss: 0.22121 val_acc: 0.96000\n",
            "Epoch [7250/10000], loss: 0.23763 acc: 0.90667 val_loss: 0.22110 val_acc: 0.96000\n",
            "Epoch [7260/10000], loss: 0.23753 acc: 0.90667 val_loss: 0.22099 val_acc: 0.96000\n",
            "Epoch [7270/10000], loss: 0.23742 acc: 0.90667 val_loss: 0.22088 val_acc: 0.96000\n",
            "Epoch [7280/10000], loss: 0.23732 acc: 0.90667 val_loss: 0.22078 val_acc: 0.96000\n",
            "Epoch [7290/10000], loss: 0.23722 acc: 0.90667 val_loss: 0.22067 val_acc: 0.96000\n",
            "Epoch [7300/10000], loss: 0.23712 acc: 0.90667 val_loss: 0.22056 val_acc: 0.96000\n",
            "Epoch [7310/10000], loss: 0.23701 acc: 0.90667 val_loss: 0.22045 val_acc: 0.96000\n",
            "Epoch [7320/10000], loss: 0.23691 acc: 0.90667 val_loss: 0.22034 val_acc: 0.96000\n",
            "Epoch [7330/10000], loss: 0.23681 acc: 0.90667 val_loss: 0.22024 val_acc: 0.96000\n",
            "Epoch [7340/10000], loss: 0.23671 acc: 0.90667 val_loss: 0.22013 val_acc: 0.96000\n",
            "Epoch [7350/10000], loss: 0.23661 acc: 0.90667 val_loss: 0.22002 val_acc: 0.96000\n",
            "Epoch [7360/10000], loss: 0.23651 acc: 0.90667 val_loss: 0.21992 val_acc: 0.96000\n",
            "Epoch [7370/10000], loss: 0.23640 acc: 0.90667 val_loss: 0.21981 val_acc: 0.96000\n",
            "Epoch [7380/10000], loss: 0.23630 acc: 0.90667 val_loss: 0.21970 val_acc: 0.96000\n",
            "Epoch [7390/10000], loss: 0.23620 acc: 0.90667 val_loss: 0.21960 val_acc: 0.96000\n",
            "Epoch [7400/10000], loss: 0.23610 acc: 0.90667 val_loss: 0.21949 val_acc: 0.96000\n",
            "Epoch [7410/10000], loss: 0.23600 acc: 0.90667 val_loss: 0.21939 val_acc: 0.96000\n",
            "Epoch [7420/10000], loss: 0.23590 acc: 0.90667 val_loss: 0.21928 val_acc: 0.96000\n",
            "Epoch [7430/10000], loss: 0.23580 acc: 0.90667 val_loss: 0.21918 val_acc: 0.96000\n",
            "Epoch [7440/10000], loss: 0.23570 acc: 0.90667 val_loss: 0.21907 val_acc: 0.96000\n",
            "Epoch [7450/10000], loss: 0.23560 acc: 0.90667 val_loss: 0.21897 val_acc: 0.96000\n",
            "Epoch [7460/10000], loss: 0.23551 acc: 0.90667 val_loss: 0.21886 val_acc: 0.96000\n",
            "Epoch [7470/10000], loss: 0.23541 acc: 0.90667 val_loss: 0.21876 val_acc: 0.96000\n",
            "Epoch [7480/10000], loss: 0.23531 acc: 0.90667 val_loss: 0.21865 val_acc: 0.96000\n",
            "Epoch [7490/10000], loss: 0.23521 acc: 0.90667 val_loss: 0.21855 val_acc: 0.96000\n",
            "Epoch [7500/10000], loss: 0.23511 acc: 0.90667 val_loss: 0.21845 val_acc: 0.96000\n",
            "Epoch [7510/10000], loss: 0.23501 acc: 0.90667 val_loss: 0.21834 val_acc: 0.96000\n",
            "Epoch [7520/10000], loss: 0.23492 acc: 0.90667 val_loss: 0.21824 val_acc: 0.96000\n",
            "Epoch [7530/10000], loss: 0.23482 acc: 0.90667 val_loss: 0.21814 val_acc: 0.96000\n",
            "Epoch [7540/10000], loss: 0.23472 acc: 0.90667 val_loss: 0.21803 val_acc: 0.96000\n",
            "Epoch [7550/10000], loss: 0.23462 acc: 0.90667 val_loss: 0.21793 val_acc: 0.96000\n",
            "Epoch [7560/10000], loss: 0.23453 acc: 0.90667 val_loss: 0.21783 val_acc: 0.96000\n",
            "Epoch [7570/10000], loss: 0.23443 acc: 0.90667 val_loss: 0.21773 val_acc: 0.96000\n",
            "Epoch [7580/10000], loss: 0.23433 acc: 0.90667 val_loss: 0.21762 val_acc: 0.96000\n",
            "Epoch [7590/10000], loss: 0.23424 acc: 0.90667 val_loss: 0.21752 val_acc: 0.96000\n",
            "Epoch [7600/10000], loss: 0.23414 acc: 0.90667 val_loss: 0.21742 val_acc: 0.96000\n",
            "Epoch [7610/10000], loss: 0.23405 acc: 0.90667 val_loss: 0.21732 val_acc: 0.96000\n",
            "Epoch [7620/10000], loss: 0.23395 acc: 0.90667 val_loss: 0.21722 val_acc: 0.96000\n",
            "Epoch [7630/10000], loss: 0.23385 acc: 0.90667 val_loss: 0.21712 val_acc: 0.96000\n",
            "Epoch [7640/10000], loss: 0.23376 acc: 0.90667 val_loss: 0.21702 val_acc: 0.96000\n",
            "Epoch [7650/10000], loss: 0.23366 acc: 0.90667 val_loss: 0.21692 val_acc: 0.96000\n",
            "Epoch [7660/10000], loss: 0.23357 acc: 0.90667 val_loss: 0.21682 val_acc: 0.96000\n",
            "Epoch [7670/10000], loss: 0.23348 acc: 0.90667 val_loss: 0.21672 val_acc: 0.96000\n",
            "Epoch [7680/10000], loss: 0.23338 acc: 0.90667 val_loss: 0.21662 val_acc: 0.96000\n",
            "Epoch [7690/10000], loss: 0.23329 acc: 0.90667 val_loss: 0.21652 val_acc: 0.96000\n",
            "Epoch [7700/10000], loss: 0.23319 acc: 0.90667 val_loss: 0.21642 val_acc: 0.96000\n",
            "Epoch [7710/10000], loss: 0.23310 acc: 0.90667 val_loss: 0.21632 val_acc: 0.96000\n",
            "Epoch [7720/10000], loss: 0.23301 acc: 0.90667 val_loss: 0.21622 val_acc: 0.96000\n",
            "Epoch [7730/10000], loss: 0.23291 acc: 0.90667 val_loss: 0.21612 val_acc: 0.96000\n",
            "Epoch [7740/10000], loss: 0.23282 acc: 0.90667 val_loss: 0.21602 val_acc: 0.96000\n",
            "Epoch [7750/10000], loss: 0.23273 acc: 0.90667 val_loss: 0.21592 val_acc: 0.96000\n",
            "Epoch [7760/10000], loss: 0.23263 acc: 0.90667 val_loss: 0.21582 val_acc: 0.96000\n",
            "Epoch [7770/10000], loss: 0.23254 acc: 0.90667 val_loss: 0.21573 val_acc: 0.96000\n",
            "Epoch [7780/10000], loss: 0.23245 acc: 0.90667 val_loss: 0.21563 val_acc: 0.96000\n",
            "Epoch [7790/10000], loss: 0.23236 acc: 0.90667 val_loss: 0.21553 val_acc: 0.96000\n",
            "Epoch [7800/10000], loss: 0.23226 acc: 0.90667 val_loss: 0.21543 val_acc: 0.96000\n",
            "Epoch [7810/10000], loss: 0.23217 acc: 0.90667 val_loss: 0.21534 val_acc: 0.96000\n",
            "Epoch [7820/10000], loss: 0.23208 acc: 0.90667 val_loss: 0.21524 val_acc: 0.96000\n",
            "Epoch [7830/10000], loss: 0.23199 acc: 0.90667 val_loss: 0.21514 val_acc: 0.96000\n",
            "Epoch [7840/10000], loss: 0.23190 acc: 0.90667 val_loss: 0.21505 val_acc: 0.96000\n",
            "Epoch [7850/10000], loss: 0.23181 acc: 0.90667 val_loss: 0.21495 val_acc: 0.96000\n",
            "Epoch [7860/10000], loss: 0.23172 acc: 0.90667 val_loss: 0.21485 val_acc: 0.96000\n",
            "Epoch [7870/10000], loss: 0.23162 acc: 0.90667 val_loss: 0.21476 val_acc: 0.96000\n",
            "Epoch [7880/10000], loss: 0.23153 acc: 0.90667 val_loss: 0.21466 val_acc: 0.96000\n",
            "Epoch [7890/10000], loss: 0.23144 acc: 0.90667 val_loss: 0.21457 val_acc: 0.96000\n",
            "Epoch [7900/10000], loss: 0.23135 acc: 0.90667 val_loss: 0.21447 val_acc: 0.96000\n",
            "Epoch [7910/10000], loss: 0.23126 acc: 0.90667 val_loss: 0.21437 val_acc: 0.96000\n",
            "Epoch [7920/10000], loss: 0.23117 acc: 0.90667 val_loss: 0.21428 val_acc: 0.96000\n",
            "Epoch [7930/10000], loss: 0.23108 acc: 0.90667 val_loss: 0.21418 val_acc: 0.96000\n",
            "Epoch [7940/10000], loss: 0.23099 acc: 0.90667 val_loss: 0.21409 val_acc: 0.96000\n",
            "Epoch [7950/10000], loss: 0.23091 acc: 0.90667 val_loss: 0.21400 val_acc: 0.96000\n",
            "Epoch [7960/10000], loss: 0.23082 acc: 0.90667 val_loss: 0.21390 val_acc: 0.96000\n",
            "Epoch [7970/10000], loss: 0.23073 acc: 0.90667 val_loss: 0.21381 val_acc: 0.96000\n",
            "Epoch [7980/10000], loss: 0.23064 acc: 0.90667 val_loss: 0.21371 val_acc: 0.96000\n",
            "Epoch [7990/10000], loss: 0.23055 acc: 0.90667 val_loss: 0.21362 val_acc: 0.96000\n",
            "Epoch [8000/10000], loss: 0.23046 acc: 0.90667 val_loss: 0.21353 val_acc: 0.96000\n",
            "Epoch [8010/10000], loss: 0.23037 acc: 0.90667 val_loss: 0.21343 val_acc: 0.96000\n",
            "Epoch [8020/10000], loss: 0.23029 acc: 0.90667 val_loss: 0.21334 val_acc: 0.96000\n",
            "Epoch [8030/10000], loss: 0.23020 acc: 0.90667 val_loss: 0.21325 val_acc: 0.96000\n",
            "Epoch [8040/10000], loss: 0.23011 acc: 0.90667 val_loss: 0.21315 val_acc: 0.96000\n",
            "Epoch [8050/10000], loss: 0.23002 acc: 0.90667 val_loss: 0.21306 val_acc: 0.96000\n",
            "Epoch [8060/10000], loss: 0.22994 acc: 0.90667 val_loss: 0.21297 val_acc: 0.96000\n",
            "Epoch [8070/10000], loss: 0.22985 acc: 0.90667 val_loss: 0.21288 val_acc: 0.96000\n",
            "Epoch [8080/10000], loss: 0.22976 acc: 0.90667 val_loss: 0.21278 val_acc: 0.96000\n",
            "Epoch [8090/10000], loss: 0.22968 acc: 0.90667 val_loss: 0.21269 val_acc: 0.96000\n",
            "Epoch [8100/10000], loss: 0.22959 acc: 0.90667 val_loss: 0.21260 val_acc: 0.96000\n",
            "Epoch [8110/10000], loss: 0.22950 acc: 0.90667 val_loss: 0.21251 val_acc: 0.96000\n",
            "Epoch [8120/10000], loss: 0.22942 acc: 0.90667 val_loss: 0.21242 val_acc: 0.96000\n",
            "Epoch [8130/10000], loss: 0.22933 acc: 0.90667 val_loss: 0.21233 val_acc: 0.96000\n",
            "Epoch [8140/10000], loss: 0.22925 acc: 0.90667 val_loss: 0.21223 val_acc: 0.96000\n",
            "Epoch [8150/10000], loss: 0.22916 acc: 0.90667 val_loss: 0.21214 val_acc: 0.96000\n",
            "Epoch [8160/10000], loss: 0.22907 acc: 0.90667 val_loss: 0.21205 val_acc: 0.96000\n",
            "Epoch [8170/10000], loss: 0.22899 acc: 0.90667 val_loss: 0.21196 val_acc: 0.96000\n",
            "Epoch [8180/10000], loss: 0.22890 acc: 0.90667 val_loss: 0.21187 val_acc: 0.96000\n",
            "Epoch [8190/10000], loss: 0.22882 acc: 0.90667 val_loss: 0.21178 val_acc: 0.96000\n",
            "Epoch [8200/10000], loss: 0.22873 acc: 0.90667 val_loss: 0.21169 val_acc: 0.96000\n",
            "Epoch [8210/10000], loss: 0.22865 acc: 0.90667 val_loss: 0.21160 val_acc: 0.96000\n",
            "Epoch [8220/10000], loss: 0.22857 acc: 0.90667 val_loss: 0.21151 val_acc: 0.96000\n",
            "Epoch [8230/10000], loss: 0.22848 acc: 0.90667 val_loss: 0.21142 val_acc: 0.96000\n",
            "Epoch [8240/10000], loss: 0.22840 acc: 0.90667 val_loss: 0.21133 val_acc: 0.96000\n",
            "Epoch [8250/10000], loss: 0.22831 acc: 0.90667 val_loss: 0.21124 val_acc: 0.96000\n",
            "Epoch [8260/10000], loss: 0.22823 acc: 0.90667 val_loss: 0.21115 val_acc: 0.96000\n",
            "Epoch [8270/10000], loss: 0.22815 acc: 0.90667 val_loss: 0.21107 val_acc: 0.96000\n",
            "Epoch [8280/10000], loss: 0.22806 acc: 0.90667 val_loss: 0.21098 val_acc: 0.96000\n",
            "Epoch [8290/10000], loss: 0.22798 acc: 0.90667 val_loss: 0.21089 val_acc: 0.96000\n",
            "Epoch [8300/10000], loss: 0.22790 acc: 0.90667 val_loss: 0.21080 val_acc: 0.96000\n",
            "Epoch [8310/10000], loss: 0.22781 acc: 0.90667 val_loss: 0.21071 val_acc: 0.96000\n",
            "Epoch [8320/10000], loss: 0.22773 acc: 0.90667 val_loss: 0.21062 val_acc: 0.96000\n",
            "Epoch [8330/10000], loss: 0.22765 acc: 0.90667 val_loss: 0.21054 val_acc: 0.96000\n",
            "Epoch [8340/10000], loss: 0.22757 acc: 0.90667 val_loss: 0.21045 val_acc: 0.96000\n",
            "Epoch [8350/10000], loss: 0.22748 acc: 0.90667 val_loss: 0.21036 val_acc: 0.96000\n",
            "Epoch [8360/10000], loss: 0.22740 acc: 0.90667 val_loss: 0.21027 val_acc: 0.96000\n",
            "Epoch [8370/10000], loss: 0.22732 acc: 0.90667 val_loss: 0.21019 val_acc: 0.96000\n",
            "Epoch [8380/10000], loss: 0.22724 acc: 0.90667 val_loss: 0.21010 val_acc: 0.96000\n",
            "Epoch [8390/10000], loss: 0.22716 acc: 0.90667 val_loss: 0.21001 val_acc: 0.96000\n",
            "Epoch [8400/10000], loss: 0.22708 acc: 0.90667 val_loss: 0.20993 val_acc: 0.96000\n",
            "Epoch [8410/10000], loss: 0.22699 acc: 0.90667 val_loss: 0.20984 val_acc: 0.96000\n",
            "Epoch [8420/10000], loss: 0.22691 acc: 0.90667 val_loss: 0.20975 val_acc: 0.96000\n",
            "Epoch [8430/10000], loss: 0.22683 acc: 0.90667 val_loss: 0.20967 val_acc: 0.96000\n",
            "Epoch [8440/10000], loss: 0.22675 acc: 0.90667 val_loss: 0.20958 val_acc: 0.96000\n",
            "Epoch [8450/10000], loss: 0.22667 acc: 0.90667 val_loss: 0.20949 val_acc: 0.96000\n",
            "Epoch [8460/10000], loss: 0.22659 acc: 0.90667 val_loss: 0.20941 val_acc: 0.96000\n",
            "Epoch [8470/10000], loss: 0.22651 acc: 0.90667 val_loss: 0.20932 val_acc: 0.96000\n",
            "Epoch [8480/10000], loss: 0.22643 acc: 0.90667 val_loss: 0.20924 val_acc: 0.96000\n",
            "Epoch [8490/10000], loss: 0.22635 acc: 0.90667 val_loss: 0.20915 val_acc: 0.96000\n",
            "Epoch [8500/10000], loss: 0.22627 acc: 0.90667 val_loss: 0.20907 val_acc: 0.96000\n",
            "Epoch [8510/10000], loss: 0.22619 acc: 0.90667 val_loss: 0.20898 val_acc: 0.96000\n",
            "Epoch [8520/10000], loss: 0.22611 acc: 0.90667 val_loss: 0.20890 val_acc: 0.96000\n",
            "Epoch [8530/10000], loss: 0.22603 acc: 0.90667 val_loss: 0.20881 val_acc: 0.96000\n",
            "Epoch [8540/10000], loss: 0.22595 acc: 0.90667 val_loss: 0.20873 val_acc: 0.96000\n",
            "Epoch [8550/10000], loss: 0.22587 acc: 0.90667 val_loss: 0.20865 val_acc: 0.96000\n",
            "Epoch [8560/10000], loss: 0.22579 acc: 0.90667 val_loss: 0.20856 val_acc: 0.96000\n",
            "Epoch [8570/10000], loss: 0.22572 acc: 0.90667 val_loss: 0.20848 val_acc: 0.96000\n",
            "Epoch [8580/10000], loss: 0.22564 acc: 0.90667 val_loss: 0.20839 val_acc: 0.96000\n",
            "Epoch [8590/10000], loss: 0.22556 acc: 0.90667 val_loss: 0.20831 val_acc: 0.96000\n",
            "Epoch [8600/10000], loss: 0.22548 acc: 0.90667 val_loss: 0.20823 val_acc: 0.96000\n",
            "Epoch [8610/10000], loss: 0.22540 acc: 0.90667 val_loss: 0.20814 val_acc: 0.96000\n",
            "Epoch [8620/10000], loss: 0.22532 acc: 0.90667 val_loss: 0.20806 val_acc: 0.96000\n",
            "Epoch [8630/10000], loss: 0.22525 acc: 0.90667 val_loss: 0.20798 val_acc: 0.96000\n",
            "Epoch [8640/10000], loss: 0.22517 acc: 0.90667 val_loss: 0.20789 val_acc: 0.96000\n",
            "Epoch [8650/10000], loss: 0.22509 acc: 0.90667 val_loss: 0.20781 val_acc: 0.96000\n",
            "Epoch [8660/10000], loss: 0.22501 acc: 0.90667 val_loss: 0.20773 val_acc: 0.96000\n",
            "Epoch [8670/10000], loss: 0.22494 acc: 0.90667 val_loss: 0.20765 val_acc: 0.96000\n",
            "Epoch [8680/10000], loss: 0.22486 acc: 0.90667 val_loss: 0.20756 val_acc: 0.96000\n",
            "Epoch [8690/10000], loss: 0.22478 acc: 0.90667 val_loss: 0.20748 val_acc: 0.96000\n",
            "Epoch [8700/10000], loss: 0.22471 acc: 0.90667 val_loss: 0.20740 val_acc: 0.96000\n",
            "Epoch [8710/10000], loss: 0.22463 acc: 0.90667 val_loss: 0.20732 val_acc: 0.96000\n",
            "Epoch [8720/10000], loss: 0.22455 acc: 0.90667 val_loss: 0.20724 val_acc: 0.96000\n",
            "Epoch [8730/10000], loss: 0.22448 acc: 0.90667 val_loss: 0.20716 val_acc: 0.96000\n",
            "Epoch [8740/10000], loss: 0.22440 acc: 0.90667 val_loss: 0.20707 val_acc: 0.96000\n",
            "Epoch [8750/10000], loss: 0.22432 acc: 0.90667 val_loss: 0.20699 val_acc: 0.96000\n",
            "Epoch [8760/10000], loss: 0.22425 acc: 0.90667 val_loss: 0.20691 val_acc: 0.96000\n",
            "Epoch [8770/10000], loss: 0.22417 acc: 0.90667 val_loss: 0.20683 val_acc: 0.96000\n",
            "Epoch [8780/10000], loss: 0.22410 acc: 0.90667 val_loss: 0.20675 val_acc: 0.96000\n",
            "Epoch [8790/10000], loss: 0.22402 acc: 0.90667 val_loss: 0.20667 val_acc: 0.96000\n",
            "Epoch [8800/10000], loss: 0.22395 acc: 0.90667 val_loss: 0.20659 val_acc: 0.96000\n",
            "Epoch [8810/10000], loss: 0.22387 acc: 0.90667 val_loss: 0.20651 val_acc: 0.96000\n",
            "Epoch [8820/10000], loss: 0.22380 acc: 0.90667 val_loss: 0.20643 val_acc: 0.96000\n",
            "Epoch [8830/10000], loss: 0.22372 acc: 0.90667 val_loss: 0.20635 val_acc: 0.96000\n",
            "Epoch [8840/10000], loss: 0.22365 acc: 0.90667 val_loss: 0.20627 val_acc: 0.96000\n",
            "Epoch [8850/10000], loss: 0.22357 acc: 0.90667 val_loss: 0.20619 val_acc: 0.96000\n",
            "Epoch [8860/10000], loss: 0.22350 acc: 0.90667 val_loss: 0.20611 val_acc: 0.96000\n",
            "Epoch [8870/10000], loss: 0.22342 acc: 0.90667 val_loss: 0.20603 val_acc: 0.96000\n",
            "Epoch [8880/10000], loss: 0.22335 acc: 0.90667 val_loss: 0.20595 val_acc: 0.96000\n",
            "Epoch [8890/10000], loss: 0.22327 acc: 0.90667 val_loss: 0.20587 val_acc: 0.96000\n",
            "Epoch [8900/10000], loss: 0.22320 acc: 0.90667 val_loss: 0.20579 val_acc: 0.96000\n",
            "Epoch [8910/10000], loss: 0.22313 acc: 0.90667 val_loss: 0.20571 val_acc: 0.96000\n",
            "Epoch [8920/10000], loss: 0.22305 acc: 0.90667 val_loss: 0.20563 val_acc: 0.96000\n",
            "Epoch [8930/10000], loss: 0.22298 acc: 0.90667 val_loss: 0.20556 val_acc: 0.96000\n",
            "Epoch [8940/10000], loss: 0.22291 acc: 0.90667 val_loss: 0.20548 val_acc: 0.96000\n",
            "Epoch [8950/10000], loss: 0.22283 acc: 0.90667 val_loss: 0.20540 val_acc: 0.96000\n",
            "Epoch [8960/10000], loss: 0.22276 acc: 0.90667 val_loss: 0.20532 val_acc: 0.96000\n",
            "Epoch [8970/10000], loss: 0.22269 acc: 0.90667 val_loss: 0.20524 val_acc: 0.96000\n",
            "Epoch [8980/10000], loss: 0.22261 acc: 0.90667 val_loss: 0.20517 val_acc: 0.96000\n",
            "Epoch [8990/10000], loss: 0.22254 acc: 0.90667 val_loss: 0.20509 val_acc: 0.96000\n",
            "Epoch [9000/10000], loss: 0.22247 acc: 0.90667 val_loss: 0.20501 val_acc: 0.96000\n",
            "Epoch [9010/10000], loss: 0.22240 acc: 0.90667 val_loss: 0.20493 val_acc: 0.96000\n",
            "Epoch [9020/10000], loss: 0.22232 acc: 0.90667 val_loss: 0.20485 val_acc: 0.96000\n",
            "Epoch [9030/10000], loss: 0.22225 acc: 0.90667 val_loss: 0.20478 val_acc: 0.96000\n",
            "Epoch [9040/10000], loss: 0.22218 acc: 0.90667 val_loss: 0.20470 val_acc: 0.96000\n",
            "Epoch [9050/10000], loss: 0.22211 acc: 0.90667 val_loss: 0.20462 val_acc: 0.96000\n",
            "Epoch [9060/10000], loss: 0.22204 acc: 0.90667 val_loss: 0.20455 val_acc: 0.96000\n",
            "Epoch [9070/10000], loss: 0.22196 acc: 0.90667 val_loss: 0.20447 val_acc: 0.96000\n",
            "Epoch [9080/10000], loss: 0.22189 acc: 0.90667 val_loss: 0.20439 val_acc: 0.96000\n",
            "Epoch [9090/10000], loss: 0.22182 acc: 0.90667 val_loss: 0.20432 val_acc: 0.96000\n",
            "Epoch [9100/10000], loss: 0.22175 acc: 0.90667 val_loss: 0.20424 val_acc: 0.96000\n",
            "Epoch [9110/10000], loss: 0.22168 acc: 0.90667 val_loss: 0.20416 val_acc: 0.96000\n",
            "Epoch [9120/10000], loss: 0.22161 acc: 0.90667 val_loss: 0.20409 val_acc: 0.96000\n",
            "Epoch [9130/10000], loss: 0.22154 acc: 0.90667 val_loss: 0.20401 val_acc: 0.96000\n",
            "Epoch [9140/10000], loss: 0.22147 acc: 0.90667 val_loss: 0.20394 val_acc: 0.96000\n",
            "Epoch [9150/10000], loss: 0.22140 acc: 0.90667 val_loss: 0.20386 val_acc: 0.96000\n",
            "Epoch [9160/10000], loss: 0.22133 acc: 0.90667 val_loss: 0.20379 val_acc: 0.96000\n",
            "Epoch [9170/10000], loss: 0.22126 acc: 0.90667 val_loss: 0.20371 val_acc: 0.96000\n",
            "Epoch [9180/10000], loss: 0.22118 acc: 0.90667 val_loss: 0.20364 val_acc: 0.96000\n",
            "Epoch [9190/10000], loss: 0.22111 acc: 0.90667 val_loss: 0.20356 val_acc: 0.96000\n",
            "Epoch [9200/10000], loss: 0.22105 acc: 0.90667 val_loss: 0.20349 val_acc: 0.96000\n",
            "Epoch [9210/10000], loss: 0.22098 acc: 0.90667 val_loss: 0.20341 val_acc: 0.96000\n",
            "Epoch [9220/10000], loss: 0.22091 acc: 0.90667 val_loss: 0.20334 val_acc: 0.96000\n",
            "Epoch [9230/10000], loss: 0.22084 acc: 0.90667 val_loss: 0.20326 val_acc: 0.96000\n",
            "Epoch [9240/10000], loss: 0.22077 acc: 0.90667 val_loss: 0.20319 val_acc: 0.96000\n",
            "Epoch [9250/10000], loss: 0.22070 acc: 0.90667 val_loss: 0.20311 val_acc: 0.96000\n",
            "Epoch [9260/10000], loss: 0.22063 acc: 0.90667 val_loss: 0.20304 val_acc: 0.96000\n",
            "Epoch [9270/10000], loss: 0.22056 acc: 0.90667 val_loss: 0.20296 val_acc: 0.96000\n",
            "Epoch [9280/10000], loss: 0.22049 acc: 0.90667 val_loss: 0.20289 val_acc: 0.96000\n",
            "Epoch [9290/10000], loss: 0.22042 acc: 0.90667 val_loss: 0.20282 val_acc: 0.96000\n",
            "Epoch [9300/10000], loss: 0.22035 acc: 0.90667 val_loss: 0.20274 val_acc: 0.96000\n",
            "Epoch [9310/10000], loss: 0.22028 acc: 0.90667 val_loss: 0.20267 val_acc: 0.96000\n",
            "Epoch [9320/10000], loss: 0.22022 acc: 0.90667 val_loss: 0.20260 val_acc: 0.96000\n",
            "Epoch [9330/10000], loss: 0.22015 acc: 0.90667 val_loss: 0.20252 val_acc: 0.96000\n",
            "Epoch [9340/10000], loss: 0.22008 acc: 0.90667 val_loss: 0.20245 val_acc: 0.96000\n",
            "Epoch [9350/10000], loss: 0.22001 acc: 0.90667 val_loss: 0.20238 val_acc: 0.96000\n",
            "Epoch [9360/10000], loss: 0.21994 acc: 0.90667 val_loss: 0.20230 val_acc: 0.96000\n",
            "Epoch [9370/10000], loss: 0.21988 acc: 0.90667 val_loss: 0.20223 val_acc: 0.96000\n",
            "Epoch [9380/10000], loss: 0.21981 acc: 0.90667 val_loss: 0.20216 val_acc: 0.96000\n",
            "Epoch [9390/10000], loss: 0.21974 acc: 0.90667 val_loss: 0.20209 val_acc: 0.96000\n",
            "Epoch [9400/10000], loss: 0.21967 acc: 0.90667 val_loss: 0.20201 val_acc: 0.96000\n",
            "Epoch [9410/10000], loss: 0.21961 acc: 0.90667 val_loss: 0.20194 val_acc: 0.96000\n",
            "Epoch [9420/10000], loss: 0.21954 acc: 0.90667 val_loss: 0.20187 val_acc: 0.96000\n",
            "Epoch [9430/10000], loss: 0.21947 acc: 0.90667 val_loss: 0.20180 val_acc: 0.96000\n",
            "Epoch [9440/10000], loss: 0.21940 acc: 0.90667 val_loss: 0.20173 val_acc: 0.96000\n",
            "Epoch [9450/10000], loss: 0.21934 acc: 0.90667 val_loss: 0.20165 val_acc: 0.96000\n",
            "Epoch [9460/10000], loss: 0.21927 acc: 0.90667 val_loss: 0.20158 val_acc: 0.96000\n",
            "Epoch [9470/10000], loss: 0.21920 acc: 0.90667 val_loss: 0.20151 val_acc: 0.96000\n",
            "Epoch [9480/10000], loss: 0.21914 acc: 0.90667 val_loss: 0.20144 val_acc: 0.96000\n",
            "Epoch [9490/10000], loss: 0.21907 acc: 0.90667 val_loss: 0.20137 val_acc: 0.96000\n",
            "Epoch [9500/10000], loss: 0.21901 acc: 0.90667 val_loss: 0.20130 val_acc: 0.96000\n",
            "Epoch [9510/10000], loss: 0.21894 acc: 0.90667 val_loss: 0.20123 val_acc: 0.96000\n",
            "Epoch [9520/10000], loss: 0.21887 acc: 0.90667 val_loss: 0.20115 val_acc: 0.96000\n",
            "Epoch [9530/10000], loss: 0.21881 acc: 0.90667 val_loss: 0.20108 val_acc: 0.96000\n",
            "Epoch [9540/10000], loss: 0.21874 acc: 0.90667 val_loss: 0.20101 val_acc: 0.96000\n",
            "Epoch [9550/10000], loss: 0.21868 acc: 0.90667 val_loss: 0.20094 val_acc: 0.96000\n",
            "Epoch [9560/10000], loss: 0.21861 acc: 0.90667 val_loss: 0.20087 val_acc: 0.96000\n",
            "Epoch [9570/10000], loss: 0.21854 acc: 0.90667 val_loss: 0.20080 val_acc: 0.96000\n",
            "Epoch [9580/10000], loss: 0.21848 acc: 0.90667 val_loss: 0.20073 val_acc: 0.96000\n",
            "Epoch [9590/10000], loss: 0.21841 acc: 0.90667 val_loss: 0.20066 val_acc: 0.96000\n",
            "Epoch [9600/10000], loss: 0.21835 acc: 0.90667 val_loss: 0.20059 val_acc: 0.96000\n",
            "Epoch [9610/10000], loss: 0.21828 acc: 0.90667 val_loss: 0.20052 val_acc: 0.96000\n",
            "Epoch [9620/10000], loss: 0.21822 acc: 0.90667 val_loss: 0.20045 val_acc: 0.96000\n",
            "Epoch [9630/10000], loss: 0.21815 acc: 0.90667 val_loss: 0.20038 val_acc: 0.96000\n",
            "Epoch [9640/10000], loss: 0.21809 acc: 0.90667 val_loss: 0.20031 val_acc: 0.96000\n",
            "Epoch [9650/10000], loss: 0.21803 acc: 0.90667 val_loss: 0.20024 val_acc: 0.96000\n",
            "Epoch [9660/10000], loss: 0.21796 acc: 0.90667 val_loss: 0.20017 val_acc: 0.96000\n",
            "Epoch [9670/10000], loss: 0.21790 acc: 0.90667 val_loss: 0.20010 val_acc: 0.96000\n",
            "Epoch [9680/10000], loss: 0.21783 acc: 0.90667 val_loss: 0.20004 val_acc: 0.96000\n",
            "Epoch [9690/10000], loss: 0.21777 acc: 0.90667 val_loss: 0.19997 val_acc: 0.96000\n",
            "Epoch [9700/10000], loss: 0.21770 acc: 0.90667 val_loss: 0.19990 val_acc: 0.96000\n",
            "Epoch [9710/10000], loss: 0.21764 acc: 0.90667 val_loss: 0.19983 val_acc: 0.96000\n",
            "Epoch [9720/10000], loss: 0.21758 acc: 0.90667 val_loss: 0.19976 val_acc: 0.96000\n",
            "Epoch [9730/10000], loss: 0.21751 acc: 0.90667 val_loss: 0.19969 val_acc: 0.96000\n",
            "Epoch [9740/10000], loss: 0.21745 acc: 0.90667 val_loss: 0.19962 val_acc: 0.96000\n",
            "Epoch [9750/10000], loss: 0.21739 acc: 0.90667 val_loss: 0.19956 val_acc: 0.96000\n",
            "Epoch [9760/10000], loss: 0.21732 acc: 0.90667 val_loss: 0.19949 val_acc: 0.96000\n",
            "Epoch [9770/10000], loss: 0.21726 acc: 0.90667 val_loss: 0.19942 val_acc: 0.96000\n",
            "Epoch [9780/10000], loss: 0.21720 acc: 0.90667 val_loss: 0.19935 val_acc: 0.96000\n",
            "Epoch [9790/10000], loss: 0.21713 acc: 0.90667 val_loss: 0.19928 val_acc: 0.96000\n",
            "Epoch [9800/10000], loss: 0.21707 acc: 0.90667 val_loss: 0.19922 val_acc: 0.96000\n",
            "Epoch [9810/10000], loss: 0.21701 acc: 0.90667 val_loss: 0.19915 val_acc: 0.96000\n",
            "Epoch [9820/10000], loss: 0.21695 acc: 0.90667 val_loss: 0.19908 val_acc: 0.96000\n",
            "Epoch [9830/10000], loss: 0.21688 acc: 0.90667 val_loss: 0.19901 val_acc: 0.96000\n",
            "Epoch [9840/10000], loss: 0.21682 acc: 0.90667 val_loss: 0.19895 val_acc: 0.96000\n",
            "Epoch [9850/10000], loss: 0.21676 acc: 0.90667 val_loss: 0.19888 val_acc: 0.96000\n",
            "Epoch [9860/10000], loss: 0.21670 acc: 0.90667 val_loss: 0.19881 val_acc: 0.96000\n",
            "Epoch [9870/10000], loss: 0.21663 acc: 0.90667 val_loss: 0.19874 val_acc: 0.96000\n",
            "Epoch [9880/10000], loss: 0.21657 acc: 0.90667 val_loss: 0.19868 val_acc: 0.96000\n",
            "Epoch [9890/10000], loss: 0.21651 acc: 0.90667 val_loss: 0.19861 val_acc: 0.96000\n",
            "Epoch [9900/10000], loss: 0.21645 acc: 0.90667 val_loss: 0.19854 val_acc: 0.96000\n",
            "Epoch [9910/10000], loss: 0.21639 acc: 0.90667 val_loss: 0.19848 val_acc: 0.96000\n",
            "Epoch [9920/10000], loss: 0.21633 acc: 0.90667 val_loss: 0.19841 val_acc: 0.96000\n",
            "Epoch [9930/10000], loss: 0.21626 acc: 0.90667 val_loss: 0.19835 val_acc: 0.96000\n",
            "Epoch [9940/10000], loss: 0.21620 acc: 0.90667 val_loss: 0.19828 val_acc: 0.96000\n",
            "Epoch [9950/10000], loss: 0.21614 acc: 0.90667 val_loss: 0.19821 val_acc: 0.96000\n",
            "Epoch [9960/10000], loss: 0.21608 acc: 0.90667 val_loss: 0.19815 val_acc: 0.96000\n",
            "Epoch [9970/10000], loss: 0.21602 acc: 0.90667 val_loss: 0.19808 val_acc: 0.96000\n",
            "Epoch [9980/10000], loss: 0.21596 acc: 0.90667 val_loss: 0.19802 val_acc: 0.96000\n",
            "Epoch [9990/10000], loss: 0.21590 acc: 0.90667 val_loss: 0.19795 val_acc: 0.96000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 正解データの0番目、2番目、3番目を抜き出す\n",
        "print(labels[[0, 2, 3]])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tEzCXwh2Xj9s",
        "outputId": "cdbb633c-dc91-4a0a-fd51-972b222fd2e3"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([1, 0, 2])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 該当する入力値\n",
        "i3 = inputs[[0, 2, 3],:]\n",
        "print(i3.data.numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CH7lBRMpYJrn",
        "outputId": "8295da82-e8d8-453e-d4fa-991c7c60a29f"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[6.3 4.7]\n",
            " [5.  1.6]\n",
            " [6.4 5.6]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "softmax = torch.nn.Softmax(dim=1)\n",
        "# i3 shape: (3, 2) -> o3 shape: (3, 3)\n",
        "o3 = net(i3)\n",
        "# k3 shape: (3, 3) (列方向に softmax を適用、 列方向の合計が1となる)\n",
        "k3 = softmax(o3)\n",
        "print(o3.data.numpy())\n",
        "print(k3.data.numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FQligTPDYTcN",
        "outputId": "0a79df2f-f669-41b0-909e-a83dc654fa64"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 8.807065   14.193748   12.998567  ]\n",
            " [12.826232    9.799994    0.17343801]\n",
            " [ 6.7954063  15.092802   17.1111    ]]\n",
            "[[3.5014052e-03 7.6497865e-01 2.3151995e-01]\n",
            " [9.5374250e-01 4.6254385e-02 3.0506512e-06]\n",
            " [2.9224984e-05 1.1729175e-01 8.8267905e-01]]\n"
          ]
        }
      ]
    }
  ]
}